<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">




<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <!-- Google Tag Manager - JD-20170831 --> 
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-MP366CC');</script>
    <!-- End Google Tag Manager -->

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta property="description" content="Efficient topic modelling of text semantics in Python." />
    <meta property="og:title" content="gensim: topic modelling for humans" />
    <meta property="og:description" content="Efficient topic modelling in Python" />

    
      <title>gensim: models.keyedvectors – Store and query word vectors</title>

    
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/jquery.qtip.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/anythingslider.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/>

  </head>

  <body>
    <!-- Google Tag Manager (noscript) - JD-20170831 -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MP366CC"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <div id="topwrap">
      
      <div id="top1">
        <div id="left1">
          <h1 class="h1gensim">
            <img src="../_static/images/logo-gensim_compact.png" alt="gensim logo" title="Gensim - topic modelling for humans" />
          </h1>
        </div>

        <div id="middleright">
          <div id="middle1">
            <div id="gensim"><a href="../index.html"><img src="../_static/images/gensim_compact.png" alt="gensim" title="Gensim home" /></a></div>
            <div id="tagline"><img src="../_static/images/tagline_compact.png" alt="gensim tagline" /></div>
          </div>
          <div id="right1">
            <div class="consulting-banner">
              <h3><a href="http://rare-technologies.com/">Get Expert Help</a></h3>
              <p>• machine learning, NLP, data mining</p>
              <p>• custom SW design, development, optimizations</p>
              <p>• corporate trainings &amp; IT consulting</p>
            </div>
          </div>
        </div>
      </div>
     

      
      <div id="menu">
        <div id="indentation1">
          <ul class="menubuttons">
            <li class="menubutton"><a href="../index.html">Home</a></li>
            <li class="menubutton"><a href="../tutorial.html">Tutorials</a></li>
            <li class="menubutton"><a href="../install.html">Install</a></li>
            <li class="menubutton"><a href="../support.html">Support</a></li>
            <li class="menubutton"><a href="../apiref.html">API</a></li>
            <li class="menubutton"><a href="../about.html">About</a></li>
          </ul>
        </div>
      </div>
      

      <div class="clearer"></div>
    </div>

    
  <script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
    URL_ROOT: '../',
    VERSION: '3.1.0',
    COLLAPSE_INDEX: false,
    FILE_SUFFIX: '.html',
    HAS_SOURCE: true
  };
  </script>
    <script type="text/javascript" src="../_static/js/jquery-1.9.1.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery.qtip.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-migrate-1.1.1.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery.anythingslider.min.js"></script>

    
    <div class="document">
      
        <div id="thinbanner">
          <div id="bodythinbanner">
            <span class="h2gensim">models.keyedvectors – Store and query word vectors</span>
          </div>
        </div>
        <div class="obsah">
          <div class="obsahwrapper">
            
  <div class="section" id="module-gensim.models.keyedvectors">
<span id="models-keyedvectors-store-and-query-word-vectors"></span><h1><code class="xref py py-mod docutils literal"><span class="pre">models.keyedvectors</span></code> – Store and query word vectors<a class="headerlink" href="#module-gensim.models.keyedvectors" title="Permalink to this headline">¶</a></h1>
<p>Word vector storage and similarity look-ups. Common code independent of the way the vectors are trained(Word2Vec, FastText, WordRank, VarEmbed etc)</p>
<p>The word vectors are considered read-only in this class.</p>
<p>Initialize the vectors by training e.g. Word2Vec:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_vectors</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span>
</pre></div>
</div>
<p>Persist the word vectors to disk with:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">word_vectors</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_vectors</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>
</pre></div>
</div>
<p>The vectors can also be instantiated from an existing file on disk in the original Google’s word2vec C format as a KeyedVectors instance:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">gensim.models.keyedvectors</span> <span class="k">import</span> <span class="n">KeyedVectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_vectors</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;/tmp/vectors.txt&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># C text format</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_vectors</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;/tmp/vectors.bin&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># C binary format</span>
</pre></div>
</div>
<p>You can perform various syntactic/semantic NLP word tasks with the vectors. Some of them
are already built-in:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">word_vectors</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;king&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;man&#39;</span><span class="p">])</span>
<span class="go">[(&#39;queen&#39;, 0.50882536), ...]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">word_vectors</span><span class="o">.</span><span class="n">most_similar_cosmul</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;king&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;man&#39;</span><span class="p">])</span>
<span class="go">[(&#39;queen&#39;, 0.71382287), ...]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">word_vectors</span><span class="o">.</span><span class="n">doesnt_match</span><span class="p">(</span><span class="s2">&quot;breakfast cereal dinner lunch&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="go">&#39;cereal&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">word_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;man&#39;</span><span class="p">)</span>
<span class="go">0.73723527</span>
</pre></div>
</div>
<p>Correlation with human opinion on word similarity:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">word_vectors</span><span class="o">.</span><span class="n">evaluate_word_pairs</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">module_path</span><span class="p">,</span> <span class="s1">&#39;test_data&#39;</span><span class="p">,</span><span class="s1">&#39;wordsim353.tsv&#39;</span><span class="p">))</span>
<span class="go">0.51, 0.62, 0.13</span>
</pre></div>
</div>
<p>And on analogies:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">word_vectors</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">module_path</span><span class="p">,</span> <span class="s1">&#39;test_data&#39;</span><span class="p">,</span> <span class="s1">&#39;questions-words.txt&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>and so on.</p>
<dl class="class">
<dt id="gensim.models.keyedvectors.KeyedVectors">
<em class="property">class </em><code class="descclassname">gensim.models.keyedvectors.</code><code class="descname">KeyedVectors</code><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../utils.html#gensim.utils.SaveLoad" title="gensim.utils.SaveLoad"><code class="xref py py-class docutils literal"><span class="pre">gensim.utils.SaveLoad</span></code></a></p>
<p>Class to contain vectors and vocab for the Word2Vec training class and other w2v methods not directly
involved in training such as most_similar()</p>
<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.accuracy">
<code class="descname">accuracy</code><span class="sig-paren">(</span><em>questions</em>, <em>restrict_vocab=30000</em>, <em>most_similar=&lt;function most_similar&gt;</em>, <em>case_insensitive=True</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute accuracy of the model. <cite>questions</cite> is a filename where lines are
4-tuples of words, split into sections by “: SECTION NAME” lines.
See questions-words.txt in <a class="reference external" href="https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip">https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip</a> for an example.</p>
<p>The accuracy is reported (=printed to log and returned as a list) for each
section separately, plus there’s one aggregate summary at the end.</p>
<p>Use <cite>restrict_vocab</cite> to ignore all questions containing a word not in the first <cite>restrict_vocab</cite>
words (default 30,000). This may be meaningful if you’ve sorted the vocabulary by descending frequency.
In case <cite>case_insensitive</cite> is True, the first <cite>restrict_vocab</cite> words are taken first, and then
case normalization is performed.</p>
<p>Use <cite>case_insensitive</cite> to convert all words in questions and vocab to their uppercase form before
evaluating the accuracy (default True). Useful in case of case-mismatch between training tokens
and question words. In case of multiple case variants of a single word, the vector for the first
occurrence (also the most frequent if vocabulary is sorted) is taken.</p>
<p>This method corresponds to the <cite>compute-accuracy</cite> script of the original C word2vec.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.doesnt_match">
<code class="descname">doesnt_match</code><span class="sig-paren">(</span><em>words</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.doesnt_match" title="Permalink to this definition">¶</a></dt>
<dd><p>Which word from the given list doesn’t go with the others?</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">doesnt_match</span><span class="p">(</span><span class="s2">&quot;breakfast cereal dinner lunch&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="go">&#39;cereal&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs">
<code class="descname">evaluate_word_pairs</code><span class="sig-paren">(</span><em>pairs</em>, <em>delimiter='\t'</em>, <em>restrict_vocab=300000</em>, <em>case_insensitive=True</em>, <em>dummy4unknown=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.evaluate_word_pairs" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute correlation of the model with human similarity judgments. <cite>pairs</cite> is a filename of a dataset where
lines are 3-tuples, each consisting of a word pair and a similarity value, separated by <cite>delimiter</cite>.
An example dataset is included in Gensim (test/test_data/wordsim353.tsv). More datasets can be found at
<a class="reference external" href="http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html">http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html</a> or <a class="reference external" href="https://www.cl.cam.ac.uk/~fh295/simlex.html">https://www.cl.cam.ac.uk/~fh295/simlex.html</a>.</p>
<p>The model is evaluated using Pearson correlation coefficient and Spearman rank-order correlation coefficient
between the similarities from the dataset and the similarities produced by the model itself.
The results are printed to log and returned as a triple (pearson, spearman, ratio of pairs with unknown words).</p>
<p>Use <cite>restrict_vocab</cite> to ignore all word pairs containing a word not in the first <cite>restrict_vocab</cite>
words (default 300,000). This may be meaningful if you’ve sorted the vocabulary by descending frequency.
If <cite>case_insensitive</cite> is True, the first <cite>restrict_vocab</cite> words are taken, and then case normalization
is performed.</p>
<p>Use <cite>case_insensitive</cite> to convert all words in the pairs and vocab to their uppercase form before
evaluating the model (default True). Useful when you expect case-mismatch between training tokens
and words pairs in the dataset. If there are multiple case variants of a single word, the vector for the first
occurrence (also the most frequent if vocabulary is sorted) is taken.</p>
<p>Use <cite>dummy4unknown=True</cite> to produce zero-valued similarities for pairs with out-of-vocabulary words.
Otherwise (default False), these pairs are skipped entirely.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.get_keras_embedding">
<code class="descname">get_keras_embedding</code><span class="sig-paren">(</span><em>train_embeddings=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.get_keras_embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a Keras ‘Embedding’ layer with weights set as the Word2Vec model’s learned word embeddings</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.init_sims">
<code class="descname">init_sims</code><span class="sig-paren">(</span><em>replace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.init_sims" title="Permalink to this definition">¶</a></dt>
<dd><p>Precompute L2-normalized vectors.</p>
<p>If <cite>replace</cite> is set, forget the original vectors and only keep the normalized
ones = saves lots of memory!</p>
<p>Note that you <strong>cannot continue training</strong> after doing a replace. The model becomes
effectively read-only = you can call <cite>most_similar</cite>, <cite>similarity</cite> etc., but not <cite>train</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>fname</em>, <em>mmap=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a previously saved object from file (also see <cite>save</cite>).</p>
<p>If the object was saved with large arrays stored separately, you can load
these arrays via mmap (shared memory) using <cite>mmap=’r’</cite>. Default: don’t use
mmap, load large arrays as normal objects.</p>
<p>If the file being loaded is compressed (either ‘.gz’ or ‘.bz2’), then
<cite>mmap=None</cite> must be set.  Load will raise an <cite>IOError</cite> if this condition
is encountered.</p>
</dd></dl>

<dl class="classmethod">
<dt id="gensim.models.keyedvectors.KeyedVectors.load_word2vec_format">
<em class="property">classmethod </em><code class="descname">load_word2vec_format</code><span class="sig-paren">(</span><em>fname</em>, <em>fvocab=None</em>, <em>binary=False</em>, <em>encoding='utf8'</em>, <em>unicode_errors='strict'</em>, <em>limit=None</em>, <em>datatype=&lt;type 'numpy.float32'&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.load_word2vec_format" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the input-hidden weight matrix from the original C word2vec-tool format.</p>
<p>Note that the information stored in the file is incomplete (the binary tree is missing),
so while you can query for word similarity etc., you cannot continue training
with a model loaded this way.</p>
<p><cite>binary</cite> is a boolean indicating whether the data is in binary word2vec format.
<cite>norm_only</cite> is a boolean indicating whether to only store normalised word2vec vectors in memory.
Word counts are read from <cite>fvocab</cite> filename, if set (this is the file generated
by <cite>-save-vocab</cite> flag of the original C tool).</p>
<p>If you trained the C model using non-utf8 encoding for words, specify that
encoding in <cite>encoding</cite>.</p>
<p><cite>unicode_errors</cite>, default ‘strict’, is a string suitable to be passed as the <cite>errors</cite>
argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source
file may include word tokens truncated in the middle of a multibyte unicode character
(as is common from the original word2vec.c tool), ‘ignore’ or ‘replace’ may help.</p>
<p><cite>limit</cite> sets a maximum number of word-vectors to read from the file. The default,
None, means read all.</p>
<p><cite>datatype</cite> (experimental) can coerce dimensions to a non-default float type (such
as np.float16) to save memory. (Such types may result in much slower bulk operations
or incompatibility with optimized routines.)</p>
</dd></dl>

<dl class="staticmethod">
<dt id="gensim.models.keyedvectors.KeyedVectors.log_accuracy">
<em class="property">static </em><code class="descname">log_accuracy</code><span class="sig-paren">(</span><em>section</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.log_accuracy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="staticmethod">
<dt id="gensim.models.keyedvectors.KeyedVectors.log_evaluate_word_pairs">
<em class="property">static </em><code class="descname">log_evaluate_word_pairs</code><span class="sig-paren">(</span><em>pearson</em>, <em>spearman</em>, <em>oov</em>, <em>pairs</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.log_evaluate_word_pairs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.most_similar">
<code class="descname">most_similar</code><span class="sig-paren">(</span><em>positive=None</em>, <em>negative=None</em>, <em>topn=10</em>, <em>restrict_vocab=None</em>, <em>indexer=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.most_similar" title="Permalink to this definition">¶</a></dt>
<dd><p>Find the top-N most similar words. Positive words contribute positively towards the
similarity, negative words negatively.</p>
<p>This method computes cosine similarity between a simple mean of the projection
weight vectors of the given words and the vectors for each word in the model.
The method corresponds to the <cite>word-analogy</cite> and <cite>distance</cite> scripts in the original
word2vec implementation.</p>
<p>If topn is False, most_similar returns the vector of similarity scores.</p>
<p><cite>restrict_vocab</cite> is an optional integer which limits the range of vectors which
are searched for most-similar values. For example, restrict_vocab=10000 would
only check the first 10000 word vectors in the vocabulary order. (This may be
meaningful if you’ve sorted the vocabulary by descending frequency.)</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;king&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;man&#39;</span><span class="p">])</span>
<span class="go">[(&#39;queen&#39;, 0.50882536), ...]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.most_similar_cosmul">
<code class="descname">most_similar_cosmul</code><span class="sig-paren">(</span><em>positive=None</em>, <em>negative=None</em>, <em>topn=10</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.most_similar_cosmul" title="Permalink to this definition">¶</a></dt>
<dd><p>Find the top-N most similar words, using the multiplicative combination objective
proposed by Omer Levy and Yoav Goldberg in <a class="footnote-reference" href="#id2" id="id1">[4]</a>. Positive words still contribute
positively towards the similarity, negative words negatively, but with less
susceptibility to one large distance dominating the calculation.</p>
<p>In the common analogy-solving case, of two positive and one negative examples,
this method is equivalent to the “3CosMul” objective (equation (4)) of Levy and Goldberg.</p>
<p>Additional positive or negative examples contribute to the numerator or denominator,
respectively – a potentially sensible but untested extension of the method. (With
a single positive example, rankings will be the same as in the default most_similar.)</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">most_similar_cosmul</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;baghdad&#39;</span><span class="p">,</span> <span class="s1">&#39;england&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;london&#39;</span><span class="p">])</span>
<span class="go">[(u&#39;iraq&#39;, 0.8488819003105164), ...]</span>
</pre></div>
</div>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[4]</a></td><td>Omer Levy and Yoav Goldberg. Linguistic Regularities in Sparse and Explicit Word Representations, 2014.</td></tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.most_similar_to_given">
<code class="descname">most_similar_to_given</code><span class="sig-paren">(</span><em>w1</em>, <em>word_list</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.most_similar_to_given" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the word from word_list most similar to w1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>w1</strong> (<em>str</em>) – a word</li>
<li><strong>word_list</strong> (<em>list</em>) – list of words containing a word most similar to w1</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">the word in word_list with the highest similarity to w1</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">KeyError</span></code> – If w1 or any word in word_list is not in the vocabulary</p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">most_similar_to_given</span><span class="p">(</span><span class="s1">&#39;music&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;water&#39;</span><span class="p">,</span> <span class="s1">&#39;sound&#39;</span><span class="p">,</span> <span class="s1">&#39;backpack&#39;</span><span class="p">,</span> <span class="s1">&#39;mouse&#39;</span><span class="p">])</span>
<span class="go">&#39;sound&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">most_similar_to_given</span><span class="p">(</span><span class="s1">&#39;snake&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;food&#39;</span><span class="p">,</span> <span class="s1">&#39;pencil&#39;</span><span class="p">,</span> <span class="s1">&#39;animal&#39;</span><span class="p">,</span> <span class="s1">&#39;phone&#39;</span><span class="p">])</span>
<span class="go">&#39;animal&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.n_similarity">
<code class="descname">n_similarity</code><span class="sig-paren">(</span><em>ws1</em>, <em>ws2</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.n_similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute cosine similarity between two sets of words.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;sushi&#39;</span><span class="p">,</span> <span class="s1">&#39;shop&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;japanese&#39;</span><span class="p">,</span> <span class="s1">&#39;restaurant&#39;</span><span class="p">])</span>
<span class="go">0.61540466561049689</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;restaurant&#39;</span><span class="p">,</span> <span class="s1">&#39;japanese&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;japanese&#39;</span><span class="p">,</span> <span class="s1">&#39;restaurant&#39;</span><span class="p">])</span>
<span class="go">1.0000000000000004</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;sushi&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;restaurant&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="n">trained_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;sushi&#39;</span><span class="p">,</span> <span class="s1">&#39;restaurant&#39;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.save" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.save_word2vec_format">
<code class="descname">save_word2vec_format</code><span class="sig-paren">(</span><em>fname</em>, <em>fvocab=None</em>, <em>binary=False</em>, <em>total_vec=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.save_word2vec_format" title="Permalink to this definition">¶</a></dt>
<dd><p>Store the input-hidden weight matrix in the same format used by the original
C word2vec-tool, for compatibility.</p>
<blockquote>
<div><cite>fname</cite> is the file used to save the vectors in
<cite>fvocab</cite> is an optional file used to save the vocabulary
<cite>binary</cite> is an optional boolean indicating whether the data is to be saved
in binary word2vec format (default: False)
<cite>total_vec</cite> is an optional parameter to explicitly specify total no. of vectors
(in case word vectors are appended with document vectors afterwards)</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.similar_by_vector">
<code class="descname">similar_by_vector</code><span class="sig-paren">(</span><em>vector</em>, <em>topn=10</em>, <em>restrict_vocab=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.similar_by_vector" title="Permalink to this definition">¶</a></dt>
<dd><p>Find the top-N most similar words by vector.</p>
<p>If topn is False, similar_by_vector returns the vector of similarity scores.</p>
<p><cite>restrict_vocab</cite> is an optional integer which limits the range of vectors which
are searched for most-similar values. For example, restrict_vocab=10000 would
only check the first 10000 word vectors in the vocabulary order. (This may be
meaningful if you’ve sorted the vocabulary by descending frequency.)</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="go">[(&#39;survey&#39;, 0.9942699074745178), ...]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.similar_by_word">
<code class="descname">similar_by_word</code><span class="sig-paren">(</span><em>word</em>, <em>topn=10</em>, <em>restrict_vocab=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.similar_by_word" title="Permalink to this definition">¶</a></dt>
<dd><p>Find the top-N most similar words.</p>
<p>If topn is False, similar_by_word returns the vector of similarity scores.</p>
<p><cite>restrict_vocab</cite> is an optional integer which limits the range of vectors which
are searched for most-similar values. For example, restrict_vocab=10000 would
only check the first 10000 word vectors in the vocabulary order. (This may be
meaningful if you’ve sorted the vocabulary by descending frequency.)</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="s1">&#39;graph&#39;</span><span class="p">)</span>
<span class="go">[(&#39;user&#39;, 0.9999163150787354), ...]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.similarity">
<code class="descname">similarity</code><span class="sig-paren">(</span><em>w1</em>, <em>w2</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute cosine similarity between two words.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;man&#39;</span><span class="p">)</span>
<span class="go">0.73723527</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="s1">&#39;woman&#39;</span><span class="p">)</span>
<span class="go">1.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.wmdistance">
<code class="descname">wmdistance</code><span class="sig-paren">(</span><em>document1</em>, <em>document2</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.wmdistance" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the Word Mover’s Distance between two documents. When using this
code, please consider citing the following papers:</p>
<p>Note that if one of the documents have no words that exist in the
Word2Vec vocab, <cite>float(‘inf’)</cite> (i.e. infinity) will be returned.</p>
<p>This method only works if <cite>pyemd</cite> is installed (can be installed via pip, but requires a C compiler).</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Train word2vec model.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Some sentences to test.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentence_obama</span> <span class="o">=</span> <span class="s1">&#39;Obama speaks to the media in Illinois&#39;</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentence_president</span> <span class="o">=</span> <span class="s1">&#39;The president greets the press in Chicago&#39;</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Remove their stopwords.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="k">import</span> <span class="n">stopwords</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">stopwords</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentence_obama</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence_obama</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentence_president</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence_president</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Compute WMD.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">distance</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wmdistance</span><span class="p">(</span><span class="n">sentence_obama</span><span class="p">,</span> <span class="n">sentence_president</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.keyedvectors.KeyedVectors.word_vec">
<code class="descname">word_vec</code><span class="sig-paren">(</span><em>word</em>, <em>use_norm=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.word_vec" title="Permalink to this definition">¶</a></dt>
<dd><p>Accept a single word as input.
Returns the word’s representations in vector space, as a 1D numpy array.</p>
<p>If <cite>use_norm</cite> is True, returns the normalized word vector.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">trained_model</span><span class="p">[</span><span class="s1">&#39;office&#39;</span><span class="p">]</span>
<span class="go">array([ -1.40128313e-02, ...])</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="gensim.models.keyedvectors.KeyedVectors.wv">
<code class="descname">wv</code><a class="headerlink" href="#gensim.models.keyedvectors.KeyedVectors.wv" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="gensim.models.keyedvectors.Vocab">
<em class="property">class </em><code class="descclassname">gensim.models.keyedvectors.</code><code class="descname">Vocab</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.keyedvectors.Vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>A single vocabulary item, used internally for collecting per-word frequency/sampling info,
and for constructing binary trees (incl. both word leaves and inner nodes).</p>
</dd></dl>

</div>


          </div>
        </div>
      

      <div class="clearer"></div>
    </div>
    

    
    <div id="footer">
      <div id="footerwrapper">
        <div id="footerleft">
          <img src="../_static/images/logo-gensim.png" class="smallerlogo" alt="smaller gensim logo" />
          <a href="../index.html"><img src="../_static/images/gensim-footer.png" alt="gensim footer image" title="Gensim home" /></a>

          <div class="copyright">
            &copy; Copyright 2009-now, <a href="mailto:radimrehurek@seznam.cz" style="color:white"> Radim Řehůřek</a>
            <br />
              Last updated on Nov 06, 2017.
          </div>
        </div>

        <div id="footermiddleright">
          <div id="footermiddle">
            <ul class="navigation">
              <li><a href="../index.html">
                Home
              </a></li>
              <li>|</li>
              <li><a href="../tutorial.html">
                Tutorials
              </a></li>
              <li>|</li>
              <li><a href="../install.html">
                Install
              </a></li>
              <li>|</li>
              <li><a href="../support.html">
                Support
              </a></li>
              <li>|</li>
              <li><a href="../apiref.html">
                API
              </a></li>
              <li>|</li>
              <li><a href="../about.html">
                About
              </a></li>
            </ul>

            <div class="tweetodsazeni">
              <div class="tweet">
                <a href="https://twitter.com/radimrehurek" target="_blank" style="color: white">Tweet @RadimRehurek</a>
              </div>
            </div>

          </div>

          <div id="footerright">
            <div class="footernadpis">
              Support:
            </div>
            <div class="googlegroupsodsazeni">
              <a href="https://groups.google.com/group/gensim" class="googlegroups">
                Stay informed via gensim mailing list:
              </a>

              <form action="http://groups.google.com/group/gensim/boxsubscribe">
                <input type="text" name="email" placeholder="your@email.com" size="28" />
                <input type="submit" name="sub" value="Subscribe" />
              </form>

            </div>

            <div class="addthis_toolbox addthis_default_style addthis_32x32_style"
                addthis:title="#gensim"
                addthis:description="Efficient Topic Modelling in Python"
                style="margin:20px 0 0 0">
              <a class="addthis_button_preferred_1"></a>
              <a class="addthis_button_preferred_2"></a>
              <a class="addthis_button_preferred_3"></a>
              <a class="addthis_button_preferred_4"></a>
              <a class="addthis_button_compact"></a>
              <a class="addthis_counter addthis_bubble_style"></a>
            </div>
          </div>

        </div>
      </div>
    </div>
    

    <script type="text/javascript">
      (function() {
      var at = document.createElement('script'); at.type = 'text/javascript'; at.async = true;
      at.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 's7.addthis.com/js/250/addthis_widget.js#pubid=ra-4d738b9b1d31ccbd';
      var sat = document.getElementsByTagName('script')[0]; sat.parentNode.insertBefore(at, sat);
      })();
    </script>

  </body>
</html>