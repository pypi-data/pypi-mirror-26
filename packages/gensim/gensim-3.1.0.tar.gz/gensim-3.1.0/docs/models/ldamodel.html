<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">




<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <!-- Google Tag Manager - JD-20170831 --> 
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-MP366CC');</script>
    <!-- End Google Tag Manager -->

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta property="description" content="Efficient topic modelling of text semantics in Python." />
    <meta property="og:title" content="gensim: topic modelling for humans" />
    <meta property="og:description" content="Efficient topic modelling in Python" />

    
      <title>gensim: models.ldamodel – Latent Dirichlet Allocation</title>

    
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/jquery.qtip.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/anythingslider.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/>

  </head>

  <body>
    <!-- Google Tag Manager (noscript) - JD-20170831 -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MP366CC"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <div id="topwrap">
      
      <div id="top1">
        <div id="left1">
          <h1 class="h1gensim">
            <img src="../_static/images/logo-gensim_compact.png" alt="gensim logo" title="Gensim - topic modelling for humans" />
          </h1>
        </div>

        <div id="middleright">
          <div id="middle1">
            <div id="gensim"><a href="../index.html"><img src="../_static/images/gensim_compact.png" alt="gensim" title="Gensim home" /></a></div>
            <div id="tagline"><img src="../_static/images/tagline_compact.png" alt="gensim tagline" /></div>
          </div>
          <div id="right1">
            <div class="consulting-banner">
              <h3><a href="http://rare-technologies.com/">Get Expert Help</a></h3>
              <p>• machine learning, NLP, data mining</p>
              <p>• custom SW design, development, optimizations</p>
              <p>• corporate trainings &amp; IT consulting</p>
            </div>
          </div>
        </div>
      </div>
     

      
      <div id="menu">
        <div id="indentation1">
          <ul class="menubuttons">
            <li class="menubutton"><a href="../index.html">Home</a></li>
            <li class="menubutton"><a href="../tutorial.html">Tutorials</a></li>
            <li class="menubutton"><a href="../install.html">Install</a></li>
            <li class="menubutton"><a href="../support.html">Support</a></li>
            <li class="menubutton"><a href="../apiref.html">API</a></li>
            <li class="menubutton"><a href="../about.html">About</a></li>
          </ul>
        </div>
      </div>
      

      <div class="clearer"></div>
    </div>

    
  <script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
    URL_ROOT: '../',
    VERSION: '3.1.0',
    COLLAPSE_INDEX: false,
    FILE_SUFFIX: '.html',
    HAS_SOURCE: true
  };
  </script>
    <script type="text/javascript" src="../_static/js/jquery-1.9.1.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery.qtip.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-migrate-1.1.1.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery.anythingslider.min.js"></script>

    
    <div class="document">
      
        <div id="thinbanner">
          <div id="bodythinbanner">
            <span class="h2gensim">models.ldamodel – Latent Dirichlet Allocation</span>
          </div>
        </div>
        <div class="obsah">
          <div class="obsahwrapper">
            
  <div class="section" id="module-gensim.models.ldamodel">
<span id="models-ldamodel-latent-dirichlet-allocation"></span><h1><code class="xref py py-mod docutils literal"><span class="pre">models.ldamodel</span></code> – Latent Dirichlet Allocation<a class="headerlink" href="#module-gensim.models.ldamodel" title="Permalink to this headline">¶</a></h1>
<p><strong>For a faster implementation of LDA (parallelized for multicore machines), see</strong> <a class="reference internal" href="ldamulticore.html#module-gensim.models.ldamulticore" title="gensim.models.ldamulticore: Latent Dirichlet Allocation"><code class="xref py py-mod docutils literal"><span class="pre">gensim.models.ldamulticore</span></code></a>.</p>
<p>Latent Dirichlet Allocation (LDA) in Python.</p>
<p>This module allows both LDA model estimation from a training corpus and inference of topic
distribution on new, unseen documents. The model can also be updated with new documents
for online training.</p>
<p>The core estimation code is based on the <cite>onlineldavb.py</cite> script by M. Hoffman <a class="footnote-reference" href="#id2" id="id1">[1]</a>, see
<strong>Hoffman, Blei, Bach: Online Learning for Latent Dirichlet Allocation, NIPS 2010.</strong></p>
<p>The algorithm:</p>
<ul class="simple">
<li>is <strong>streamed</strong>: training documents may come in sequentially, no random access required,</li>
<li>runs in <strong>constant memory</strong> w.r.t. the number of documents: size of the
training corpus does not affect memory footprint, can process corpora larger than RAM, and</li>
<li>is <strong>distributed</strong>: makes use of a cluster of machines, if available, to
speed up model estimation.</li>
</ul>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td><a class="reference external" href="http://www.cs.princeton.edu/~mdhoffma">http://www.cs.princeton.edu/~mdhoffma</a></td></tr>
</tbody>
</table>
<dl class="class">
<dt id="gensim.models.ldamodel.LdaModel">
<em class="property">class </em><code class="descclassname">gensim.models.ldamodel.</code><code class="descname">LdaModel</code><span class="sig-paren">(</span><em>corpus=None</em>, <em>num_topics=100</em>, <em>id2word=None</em>, <em>distributed=False</em>, <em>chunksize=2000</em>, <em>passes=1</em>, <em>update_every=1</em>, <em>alpha='symmetric'</em>, <em>eta=None</em>, <em>decay=0.5</em>, <em>offset=1.0</em>, <em>eval_every=10</em>, <em>iterations=50</em>, <em>gamma_threshold=0.001</em>, <em>minimum_probability=0.01</em>, <em>random_state=None</em>, <em>ns_conf=None</em>, <em>minimum_phi_value=0.01</em>, <em>per_word_topics=False</em>, <em>callbacks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../interfaces.html#gensim.interfaces.TransformationABC" title="gensim.interfaces.TransformationABC"><code class="xref py py-class docutils literal"><span class="pre">gensim.interfaces.TransformationABC</span></code></a>, <a class="reference internal" href="basemodel.html#gensim.models.basemodel.BaseTopicModel" title="gensim.models.basemodel.BaseTopicModel"><code class="xref py py-class docutils literal"><span class="pre">gensim.models.basemodel.BaseTopicModel</span></code></a></p>
<p>The constructor estimates Latent Dirichlet Allocation model parameters based
on a training corpus:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span> <span class="o">=</span> <span class="n">LdaModel</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>You can then infer topic distributions on new, unseen documents, with</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">doc_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="p">[</span><span class="n">doc_bow</span><span class="p">]</span>
</pre></div>
</div>
<p>The model can be updated (trained) with new documents via</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">other_corpus</span><span class="p">)</span>
</pre></div>
</div>
<p>Model persistency is achieved through its <cite>load</cite>/<cite>save</cite> methods.</p>
<p>If given, start training from the iterable <cite>corpus</cite> straight away. If not given,
the model is left untrained (presumably because you want to call <cite>update()</cite> manually).</p>
<p><cite>num_topics</cite> is the number of requested latent topics to be extracted from
the training corpus.</p>
<p><cite>id2word</cite> is a mapping from word ids (integers) to words (strings). It is
used to determine the vocabulary size, as well as for debugging and topic
printing.</p>
<p><cite>alpha</cite> and <cite>eta</cite> are hyperparameters that affect sparsity of the document-topic
(theta) and topic-word (lambda) distributions. Both default to a symmetric
1.0/num_topics prior.</p>
<p><cite>alpha</cite> can be set to an explicit array = prior of your choice. It also
support special values of ‘asymmetric’ and ‘auto’: the former uses a fixed
normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric
prior directly from your data.</p>
<p><cite>eta</cite> can be a scalar for a symmetric prior over topic/word
distributions, or a vector of shape num_words, which can be used to
impose (user defined) asymmetric priors over the word distribution.
It also supports the special value ‘auto’, which learns an asymmetric
prior over words directly from your data. <cite>eta</cite> can also be a matrix
of shape num_topics x num_words, which can be used to impose
asymmetric priors over the word distribution on a per-topic basis
(can not be learned from data).</p>
<p>Turn on <cite>distributed</cite> to force distributed computing (see the <a class="reference external" href="http://radimrehurek.com/gensim/distributed.html">web tutorial</a>
on how to set up a cluster of machines for gensim).</p>
<p>Calculate and log perplexity estimate from the latest mini-batch every
<cite>eval_every</cite> model updates (setting this to 1 slows down training ~2x;
default is 10 for better performance). Set to None to disable perplexity estimation.</p>
<p><cite>decay</cite> and <cite>offset</cite> parameters are the same as Kappa and Tau_0 in
Hoffman et al, respectively.</p>
<p><cite>minimum_probability</cite> controls filtering the topics returned for a document (bow).</p>
<p><cite>random_state</cite> can be a np.random.RandomState object or the seed for one</p>
<p><cite>callbacks</cite> a list of metric callbacks to log/visualize evaluation metrics of topic model during training</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span> <span class="o">=</span> <span class="n">LdaModel</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># train model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">lda</span><span class="p">[</span><span class="n">doc_bow</span><span class="p">])</span> <span class="c1"># get topic probability distribution for a document</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">corpus2</span><span class="p">)</span> <span class="c1"># update the LDA model with additional documents</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">lda</span><span class="p">[</span><span class="n">doc_bow</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span> <span class="o">=</span> <span class="n">LdaModel</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">eval_every</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># train asymmetric alpha from data</span>
</pre></div>
</div>
<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.bound">
<code class="descname">bound</code><span class="sig-paren">(</span><em>corpus</em>, <em>gamma=None</em>, <em>subsample_ratio=1.0</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.bound" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate the variational bound of documents from <cite>corpus</cite>:
E_q[log p(corpus)] - E_q[log q(corpus)]</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>corpus</strong> – documents to infer variational bounds from.</li>
<li><strong>gamma</strong> – the variational parameters on topic weights for each <cite>corpus</cite>
document (=2d matrix=what comes out of <cite>inference()</cite>).
If not supplied, will be inferred from the model.</li>
<li><strong>subsample_ratio</strong> (<em>float</em>) – If <cite>corpus</cite> is a sample of the whole corpus,
pass this to inform on what proportion of the corpus it represents.
This is used as a multiplicative factor to scale the likelihood
appropriately.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The variational bound score calculated.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.clear">
<code class="descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear model state (free up some memory). Used in the distributed algo.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.diff">
<code class="descname">diff</code><span class="sig-paren">(</span><em>other</em>, <em>distance='kullback_leibler'</em>, <em>num_words=100</em>, <em>n_ann_terms=10</em>, <em>diagonal=False</em>, <em>annotation=True</em>, <em>normed=True</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.diff" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate difference topic2topic between two Lda models
<cite>other</cite> instances of <cite>LdaMulticore</cite> or <cite>LdaModel</cite>
<cite>distance</cite> is function that will be applied to calculate difference between any topic pair.
Available values: <cite>kullback_leibler</cite>, <cite>hellinger</cite>, <cite>jaccard</cite> and <cite>jensen_shannon</cite>
<cite>num_words</cite> is quantity of most relevant words that used if distance == <cite>jaccard</cite> (also used for annotation)
<cite>n_ann_terms</cite> is max quantity of words in intersection/symmetric difference between topics (used for annotation)
<cite>diagonal</cite> set to True if the difference is required only between the identical topic no.s (returns diagonal of diff matrix)
<cite>annotation</cite> whether the intersection or difference of words between two topics should be returned
Returns a matrix Z with shape (m1.num_topics, m2.num_topics), where Z[i][j] - difference between topic_i and topic_j
and matrix annotation (if True) with shape (m1.num_topics, m2.num_topics, 2, None),
where:</p>
<blockquote>
<div>annotation[i][j] = [[<cite>int_1</cite>, <cite>int_2</cite>, …], [<cite>diff_1</cite>, <cite>diff_2</cite>, …]] and
<cite>int_k</cite> is word from intersection of <cite>topic_i</cite> and <cite>topic_j</cite> and
<cite>diff_l</cite> is word from symmetric difference of <cite>topic_i</cite> and <cite>topic_j</cite>
<cite>normed</cite> is a flag. If <cite>true</cite>, matrix Z will be normalized</div></blockquote>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span> <span class="o">=</span> <span class="n">LdaMulticore</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path_1</span><span class="p">),</span> <span class="n">LdaMulticore</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path_2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mdiff</span><span class="p">,</span> <span class="n">annotation</span> <span class="o">=</span> <span class="n">m1</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">m2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">mdiff</span><span class="p">)</span> <span class="c1"># get matrix with difference for each topic pair from `m1` and `m2`</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">annotation</span><span class="p">)</span> <span class="c1"># get array with positive/negative words for each topic pair from `m1` and `m2`</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.do_estep">
<code class="descname">do_estep</code><span class="sig-paren">(</span><em>chunk</em>, <em>state=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.do_estep" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform inference on a chunk of documents, and accumulate the collected
sufficient statistics in <cite>state</cite> (or <cite>self.state</cite> if None).</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.do_mstep">
<code class="descname">do_mstep</code><span class="sig-paren">(</span><em>rho</em>, <em>other</em>, <em>extra_pass=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.do_mstep" title="Permalink to this definition">¶</a></dt>
<dd><p>M step: use linear interpolation between the existing topics and
collected sufficient statistics in <cite>other</cite> to update the topics.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.get_document_topics">
<code class="descname">get_document_topics</code><span class="sig-paren">(</span><em>bow</em>, <em>minimum_probability=None</em>, <em>minimum_phi_value=None</em>, <em>per_word_topics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.get_document_topics" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>bow</strong> (<em>list</em>) – Bag-of-words representation of the document to get topics for.</li>
<li><strong>minimum_probability</strong> (<em>float</em>) – Ignore topics with probability below this value
(None by default). If set to None, a value of 1e-8 is used to prevent 0s.</li>
<li><strong>per_word_topics</strong> (<em>bool</em>) – If True, also returns a list of topics, sorted in
descending order of most likely topics for that word. It also returns a list
of word_ids and each words corresponding topics’ phi_values, multiplied by
feature length (i.e, word count).</li>
<li><strong>minimum_phi_value</strong> (<em>float</em>) – if <cite>per_word_topics</cite> is True, this represents a lower
bound on the term probabilities that are included (None by default). If set
to None, a value of 1e-8 is used to prevent 0s.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">topic distribution for the given document <cite>bow</cite>, as a list of
<cite>(topic_id, topic_probability)</cite> 2-tuples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.get_term_topics">
<code class="descname">get_term_topics</code><span class="sig-paren">(</span><em>word_id</em>, <em>minimum_probability=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.get_term_topics" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>word_id</strong> (<em>int</em>) – ID of the word to get topic probabilities for.</li>
<li><strong>minimum_probability</strong> (<em>float</em>) – Only include topic probabilities above this
value (None by default). If set to None, use 1e-8 to prevent including 0s.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The most likely topics for the given word. Each topic is represented
as a tuple of <cite>(topic_id, term_probability)</cite>.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.get_topic_terms">
<code class="descname">get_topic_terms</code><span class="sig-paren">(</span><em>topicid</em>, <em>topn=10</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.get_topic_terms" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>topn</strong> (<em>int</em>) – Only return 2-tuples for the topn most probable words
(ignore the rest).</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><cite>(word_id, probability)</cite> 2-tuples for the most probable words
in topic with id <cite>topicid</cite>.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.get_topics">
<code class="descname">get_topics</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.get_topics" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><cite>num_topics</cite> x <cite>vocabulary_size</cite> array of floats which represents
the term topic matrix learned during inference.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">np.ndarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.inference">
<code class="descname">inference</code><span class="sig-paren">(</span><em>chunk</em>, <em>collect_sstats=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a chunk of sparse document vectors, estimate gamma (parameters
controlling the topic weights) for each document in the chunk.</p>
<p>This function does not modify the model (=is read-only aka const). The
whole input chunk of document is assumed to fit in RAM; chunking of a
large corpus must be done earlier in the pipeline.</p>
<p>If <cite>collect_sstats</cite> is True, also collect sufficient statistics needed
to update the model’s topic-word distributions, and return a 2-tuple
<cite>(gamma, sstats)</cite>. Otherwise, return <cite>(gamma, None)</cite>. <cite>gamma</cite> is of shape
<cite>len(chunk) x self.num_topics</cite>.</p>
<p>Avoids computing the <cite>phi</cite> variational parameter directly using the
optimization presented in <strong>Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001</strong>.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.init_dir_prior">
<code class="descname">init_dir_prior</code><span class="sig-paren">(</span><em>prior</em>, <em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.init_dir_prior" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="classmethod">
<dt id="gensim.models.ldamodel.LdaModel.load">
<em class="property">classmethod </em><code class="descname">load</code><span class="sig-paren">(</span><em>fname</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a previously saved object from file (also see <cite>save</cite>).</p>
<p>Large arrays can be memmap’ed back as read-only (shared memory) by setting <cite>mmap=’r’</cite>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">LdaModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="n">mmap</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.log_perplexity">
<code class="descname">log_perplexity</code><span class="sig-paren">(</span><em>chunk</em>, <em>total_docs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.log_perplexity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate and return per-word likelihood bound, using the <cite>chunk</cite> of
documents as evaluation corpus. Also output the calculated statistics. incl.
perplexity=2^(-bound), to log at INFO level.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.print_topic">
<code class="descname">print_topic</code><span class="sig-paren">(</span><em>topicno</em>, <em>topn=10</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.print_topic" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a single topic as a formatted string. See <cite>show_topic()</cite> for parameters.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lsimodel</span><span class="o">.</span><span class="n">print_topic</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">&#39;-0.340 * &quot;category&quot; + 0.298 * &quot;$M$&quot; + 0.183 * &quot;algebra&quot; + -0.174 * &quot;functor&quot; + -0.168 * &quot;operator&quot;&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.print_topics">
<code class="descname">print_topics</code><span class="sig-paren">(</span><em>num_topics=20</em>, <em>num_words=10</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.print_topics" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <cite>show_topics()</cite> that prints the <cite>num_words</cite> most
probable words for <cite>topics</cite> number of topics to log.
Set <cite>topics=-1</cite> to print all topics.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>fname</em>, <em>ignore=('state'</em>, <em>'dispatcher')</em>, <em>separately=None</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the model to file.</p>
<p>Large internal arrays may be stored into separate files, with <cite>fname</cite> as prefix.</p>
<p><cite>separately</cite> can be used to define which arrays should be stored in separate files.</p>
<p><cite>ignore</cite> parameter can be used to define which variables should be ignored, i.e. left
out from the pickled lda model. By default the internal <cite>state</cite> is ignored as it uses
its own serialisation not the one provided by <cite>LdaModel</cite>. The <cite>state</cite> and <cite>dispatcher</cite>
will be added to any ignore parameter defined.</p>
<p>Note: do not save as a compressed file if you intend to load the file back with <cite>mmap</cite>.</p>
<p>Note: If you intend to use models across Python 2/3 versions there are a few things to
keep in mind:</p>
<blockquote>
<div><ol class="arabic simple">
<li>The pickled Python dictionaries will not work across Python versions</li>
<li>The <cite>save</cite> method does not automatically save all np arrays using np, only
those ones that exceed <cite>sep_limit</cite> set in <cite>gensim.utils.SaveLoad.save</cite>. The main
concern here is the <cite>alpha</cite> array if for instance using <cite>alpha=’auto’</cite>.</li>
</ol>
</div></blockquote>
<p>Please refer to the wiki recipes section (<a class="reference external" href="https://github.com/piskvorky/gensim/wiki/Recipes-&amp;-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2">https://github.com/piskvorky/gensim/wiki/Recipes-&amp;-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2</a>)
for an example on how to work around these issues.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.show_topic">
<code class="descname">show_topic</code><span class="sig-paren">(</span><em>topicid</em>, <em>topn=10</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.show_topic" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>topn</strong> (<em>int</em>) – Only return 2-tuples for the topn most probable words
(ignore the rest).</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">of <cite>(word, probability)</cite> 2-tuples for the most probable
words in topic <cite>topicid</cite>.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.show_topics">
<code class="descname">show_topics</code><span class="sig-paren">(</span><em>num_topics=10</em>, <em>num_words=10</em>, <em>log=False</em>, <em>formatted=True</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.show_topics" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_topics</strong> (<em>int</em>) – show results for first <cite>num_topics</cite> topics.
Unlike LSA, there is no natural ordering between the topics in LDA.
The returned <cite>num_topics &lt;= self.num_topics</cite> subset of all topics is
therefore arbitrary and may change between two LDA training runs.</li>
<li><strong>num_words</strong> (<em>int</em>) – include top <cite>num_words</cite> with highest probabilities in topic.</li>
<li><strong>log</strong> (<em>bool</em>) – If True, log output in addition to returning it.</li>
<li><strong>formatted</strong> (<em>bool</em>) – If True, format topics as strings, otherwise return them as
<cite>(word, probability)</cite> 2-tuples.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><cite>num_words</cite> most significant words for <cite>num_topics</cite> number of topics
(10 words for top 10 topics, by default).</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.sync_state">
<code class="descname">sync_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.sync_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.top_topics">
<code class="descname">top_topics</code><span class="sig-paren">(</span><em>corpus=None</em>, <em>texts=None</em>, <em>dictionary=None</em>, <em>window_size=None</em>, <em>coherence='u_mass'</em>, <em>topn=20</em>, <em>processes=-1</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.top_topics" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the coherence for each topic; default is Umass coherence.</p>
<p>See the <code class="xref py py-class docutils literal"><span class="pre">gensim.models.CoherenceModel</span></code> constructor for more info on the
parameters and the different coherence metrics.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">tuples with <cite>(topic_repr, coherence_score)</cite>, where <cite>topic_repr</cite> is a list
of representations of the <cite>topn</cite> terms for the topic. The terms are represented
as tuples of <cite>(membership_in_topic, token)</cite>. The <cite>coherence_score</cite> is a float.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">list</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>corpus</em>, <em>chunksize=None</em>, <em>decay=None</em>, <em>offset=None</em>, <em>passes=None</em>, <em>update_every=None</em>, <em>eval_every=None</em>, <em>iterations=None</em>, <em>gamma_threshold=None</em>, <em>chunks_as_numpy=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Train the model with new documents, by EM-iterating over <cite>corpus</cite> until
the topics converge (or until the maximum number of allowed iterations
is reached). <cite>corpus</cite> must be an iterable (repeatable stream of documents),</p>
<p>In distributed mode, the E step is distributed over a cluster of machines.</p>
<p>This update also supports updating an already trained model (<cite>self</cite>)
with new documents from <cite>corpus</cite>; the two models are then merged in
proportion to the number of old vs. new documents. This feature is still
experimental for non-stationary input streams.</p>
<p>For stationary input (no topic drift in new documents), on the other hand,
this equals the online update of Hoffman et al. and is guaranteed to
converge for any <cite>decay</cite> in (0.5, 1.0&gt;. Additionally, for smaller
<cite>corpus</cite> sizes, an increasing <cite>offset</cite> may be beneficial (see
Table 1 in Hoffman et al.)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>corpus</strong> (<em>gensim corpus</em>) – The corpus with which the LDA model should be updated.</li>
<li><strong>chunks_as_numpy</strong> (<em>bool</em>) – Whether each chunk passed to <cite>.inference</cite> should be a np
array of not. np can in some settings turn the term IDs
into floats, these will be converted back into integers in
inference, which incurs a performance hit. For distributed
computing it may be desirable to keep the chunks as np
arrays.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>For other parameter settings, see <a class="reference internal" href="#gensim.models.ldamodel.LdaModel" title="gensim.models.ldamodel.LdaModel"><code class="xref py py-class docutils literal"><span class="pre">LdaModel</span></code></a> constructor.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.update_alpha">
<code class="descname">update_alpha</code><span class="sig-paren">(</span><em>gammat</em>, <em>rho</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.update_alpha" title="Permalink to this definition">¶</a></dt>
<dd><p>Update parameters for the Dirichlet prior on the per-document
topic weights <cite>alpha</cite> given the last <cite>gammat</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaModel.update_eta">
<code class="descname">update_eta</code><span class="sig-paren">(</span><em>lambdat</em>, <em>rho</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaModel.update_eta" title="Permalink to this definition">¶</a></dt>
<dd><p>Update parameters for the Dirichlet prior on the per-topic
word weights <cite>eta</cite> given the last <cite>lambdat</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="gensim.models.ldamodel.LdaState">
<em class="property">class </em><code class="descclassname">gensim.models.ldamodel.</code><code class="descname">LdaState</code><span class="sig-paren">(</span><em>eta</em>, <em>shape</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaState" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../utils.html#gensim.utils.SaveLoad" title="gensim.utils.SaveLoad"><code class="xref py py-class docutils literal"><span class="pre">gensim.utils.SaveLoad</span></code></a></p>
<p>Encapsulate information for distributed computation of LdaModel objects.</p>
<p>Objects of this class are sent over the network, so try to keep them lean to
reduce traffic.</p>
<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.blend">
<code class="descname">blend</code><span class="sig-paren">(</span><em>rhot</em>, <em>other</em>, <em>targetsize=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaState.blend" title="Permalink to this definition">¶</a></dt>
<dd><p>Given LdaState <cite>other</cite>, merge it with the current state. Stretch both to
<cite>targetsize</cite> documents before merging, so that they are of comparable
magnitude.</p>
<p>Merging is done by average weighting: in the extremes, <cite>rhot=0.0</cite> means
<cite>other</cite> is completely ignored; <cite>rhot=1.0</cite> means <cite>self</cite> is completely ignored.</p>
<p>This procedure corresponds to the stochastic gradient update from Hoffman
et al., algorithm 2 (eq. 14).</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.blend2">
<code class="descname">blend2</code><span class="sig-paren">(</span><em>rhot</em>, <em>other</em>, <em>targetsize=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaState.blend2" title="Permalink to this definition">¶</a></dt>
<dd><p>Alternative, more simple blend.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.get_Elogbeta">
<code class="descname">get_Elogbeta</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaState.get_Elogbeta" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.get_lambda">
<code class="descname">get_lambda</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaState.get_lambda" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>fname</em>, <em>mmap=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaState.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a previously saved object from file (also see <cite>save</cite>).</p>
<p>If the object was saved with large arrays stored separately, you can load
these arrays via mmap (shared memory) using <cite>mmap=’r’</cite>. Default: don’t use
mmap, load large arrays as normal objects.</p>
<p>If the file being loaded is compressed (either ‘.gz’ or ‘.bz2’), then
<cite>mmap=None</cite> must be set.  Load will raise an <cite>IOError</cite> if this condition
is encountered.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.merge">
<code class="descname">merge</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaState.merge" title="Permalink to this definition">¶</a></dt>
<dd><p>Merge the result of an E step from one node with that of another node
(summing up sufficient statistics).</p>
<p>The merging is trivial and after merging all cluster nodes, we have the
exact same result as if the computation was run on a single node (no
approximation).</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.reset">
<code class="descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaState.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare the state for a new EM iteration (reset sufficient stats).</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamodel.LdaState.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>fname_or_handle</em>, <em>separately=None</em>, <em>sep_limit=10485760</em>, <em>ignore=frozenset([])</em>, <em>pickle_protocol=2</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.LdaState.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the object to file (also see <cite>load</cite>).</p>
<p><cite>fname_or_handle</cite> is either a string specifying the file name to
save to, or an open file-like object which can be written to. If
the object is a file handle, no special array handling will be
performed; all attributes will be saved to the same file.</p>
<p>If <cite>separately</cite> is None, automatically detect large
numpy/scipy.sparse arrays in the object being stored, and store
them into separate files. This avoids pickle memory errors and
allows mmap’ing large arrays back on load efficiently.</p>
<p>You can also set <cite>separately</cite> manually, in which case it must be
a list of attribute names to be stored in separate files. The
automatic check is not performed in this case.</p>
<p><cite>ignore</cite> is a set of attribute names to <em>not</em> serialize (file
handles, caches etc). On subsequent load() these attributes will
be set to None.</p>
<p><cite>pickle_protocol</cite> defaults to 2 so the pickled object can be imported
in both Python 2 and 3.</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="gensim.models.ldamodel.update_dir_prior">
<code class="descclassname">gensim.models.ldamodel.</code><code class="descname">update_dir_prior</code><span class="sig-paren">(</span><em>prior</em>, <em>N</em>, <em>logphat</em>, <em>rho</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamodel.update_dir_prior" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates a given prior using Newton’s method, described in
<strong>Huang: Maximum Likelihood Estimation of Dirichlet Distribution Parameters.</strong>
<a class="reference external" href="http://jonathan-huang.org/research/dirichlet/dirichlet.pdf">http://jonathan-huang.org/research/dirichlet/dirichlet.pdf</a></p>
</dd></dl>

</div>


          </div>
        </div>
      

      <div class="clearer"></div>
    </div>
    

    
    <div id="footer">
      <div id="footerwrapper">
        <div id="footerleft">
          <img src="../_static/images/logo-gensim.png" class="smallerlogo" alt="smaller gensim logo" />
          <a href="../index.html"><img src="../_static/images/gensim-footer.png" alt="gensim footer image" title="Gensim home" /></a>

          <div class="copyright">
            &copy; Copyright 2009-now, <a href="mailto:radimrehurek@seznam.cz" style="color:white"> Radim Řehůřek</a>
            <br />
              Last updated on Nov 06, 2017.
          </div>
        </div>

        <div id="footermiddleright">
          <div id="footermiddle">
            <ul class="navigation">
              <li><a href="../index.html">
                Home
              </a></li>
              <li>|</li>
              <li><a href="../tutorial.html">
                Tutorials
              </a></li>
              <li>|</li>
              <li><a href="../install.html">
                Install
              </a></li>
              <li>|</li>
              <li><a href="../support.html">
                Support
              </a></li>
              <li>|</li>
              <li><a href="../apiref.html">
                API
              </a></li>
              <li>|</li>
              <li><a href="../about.html">
                About
              </a></li>
            </ul>

            <div class="tweetodsazeni">
              <div class="tweet">
                <a href="https://twitter.com/radimrehurek" target="_blank" style="color: white">Tweet @RadimRehurek</a>
              </div>
            </div>

          </div>

          <div id="footerright">
            <div class="footernadpis">
              Support:
            </div>
            <div class="googlegroupsodsazeni">
              <a href="https://groups.google.com/group/gensim" class="googlegroups">
                Stay informed via gensim mailing list:
              </a>

              <form action="http://groups.google.com/group/gensim/boxsubscribe">
                <input type="text" name="email" placeholder="your@email.com" size="28" />
                <input type="submit" name="sub" value="Subscribe" />
              </form>

            </div>

            <div class="addthis_toolbox addthis_default_style addthis_32x32_style"
                addthis:title="#gensim"
                addthis:description="Efficient Topic Modelling in Python"
                style="margin:20px 0 0 0">
              <a class="addthis_button_preferred_1"></a>
              <a class="addthis_button_preferred_2"></a>
              <a class="addthis_button_preferred_3"></a>
              <a class="addthis_button_preferred_4"></a>
              <a class="addthis_button_compact"></a>
              <a class="addthis_counter addthis_bubble_style"></a>
            </div>
          </div>

        </div>
      </div>
    </div>
    

    <script type="text/javascript">
      (function() {
      var at = document.createElement('script'); at.type = 'text/javascript'; at.async = true;
      at.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 's7.addthis.com/js/250/addthis_widget.js#pubid=ra-4d738b9b1d31ccbd';
      var sat = document.getElementsByTagName('script')[0]; sat.parentNode.insertBefore(at, sat);
      })();
    </script>

  </body>
</html>