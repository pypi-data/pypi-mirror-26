<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">




<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <!-- Google Tag Manager - JD-20170831 --> 
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-MP366CC');</script>
    <!-- End Google Tag Manager -->

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta property="description" content="Efficient topic modelling of text semantics in Python." />
    <meta property="og:title" content="gensim: topic modelling for humans" />
    <meta property="og:description" content="Efficient topic modelling in Python" />

    
      <title>gensim: models.ldamulticore – parallelized Latent Dirichlet Allocation</title>

    
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/jquery.qtip.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/anythingslider.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

    <link rel="shortcut icon" href="../_static/favicon.ico"/>

  </head>

  <body>
    <!-- Google Tag Manager (noscript) - JD-20170831 -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MP366CC"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <div id="topwrap">
      
      <div id="top1">
        <div id="left1">
          <h1 class="h1gensim">
            <img src="../_static/images/logo-gensim_compact.png" alt="gensim logo" title="Gensim - topic modelling for humans" />
          </h1>
        </div>

        <div id="middleright">
          <div id="middle1">
            <div id="gensim"><a href="../index.html"><img src="../_static/images/gensim_compact.png" alt="gensim" title="Gensim home" /></a></div>
            <div id="tagline"><img src="../_static/images/tagline_compact.png" alt="gensim tagline" /></div>
          </div>
          <div id="right1">
            <div class="consulting-banner">
              <h3><a href="http://rare-technologies.com/">Get Expert Help</a></h3>
              <p>• machine learning, NLP, data mining</p>
              <p>• custom SW design, development, optimizations</p>
              <p>• corporate trainings &amp; IT consulting</p>
            </div>
          </div>
        </div>
      </div>
     

      
      <div id="menu">
        <div id="indentation1">
          <ul class="menubuttons">
            <li class="menubutton"><a href="../index.html">Home</a></li>
            <li class="menubutton"><a href="../tutorial.html">Tutorials</a></li>
            <li class="menubutton"><a href="../install.html">Install</a></li>
            <li class="menubutton"><a href="../support.html">Support</a></li>
            <li class="menubutton"><a href="../apiref.html">API</a></li>
            <li class="menubutton"><a href="../about.html">About</a></li>
          </ul>
        </div>
      </div>
      

      <div class="clearer"></div>
    </div>

    
  <script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
    URL_ROOT: '../',
    VERSION: '3.1.0',
    COLLAPSE_INDEX: false,
    FILE_SUFFIX: '.html',
    HAS_SOURCE: true
  };
  </script>
    <script type="text/javascript" src="../_static/js/jquery-1.9.1.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery.qtip.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-migrate-1.1.1.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery.anythingslider.min.js"></script>

    
    <div class="document">
      
        <div id="thinbanner">
          <div id="bodythinbanner">
            <span class="h2gensim">models.ldamulticore – parallelized Latent Dirichlet Allocation</span>
          </div>
        </div>
        <div class="obsah">
          <div class="obsahwrapper">
            
  <div class="section" id="module-gensim.models.ldamulticore">
<span id="models-ldamulticore-parallelized-latent-dirichlet-allocation"></span><h1><code class="xref py py-mod docutils literal"><span class="pre">models.ldamulticore</span></code> – parallelized Latent Dirichlet Allocation<a class="headerlink" href="#module-gensim.models.ldamulticore" title="Permalink to this headline">¶</a></h1>
<p>Latent Dirichlet Allocation (LDA) in Python, using all CPU cores to parallelize and
speed up model training.</p>
<p>The parallelization uses multiprocessing; in case this doesn’t work for you for
some reason, try the <a class="reference internal" href="ldamodel.html#gensim.models.ldamodel.LdaModel" title="gensim.models.ldamodel.LdaModel"><code class="xref py py-class docutils literal"><span class="pre">gensim.models.ldamodel.LdaModel</span></code></a> class which is an
equivalent, but more straightforward and single-core implementation.</p>
<p>The training algorithm:</p>
<ul class="simple">
<li>is <strong>streamed</strong>: training documents may come in sequentially, no random access required,</li>
<li>runs in <strong>constant memory</strong> w.r.t. the number of documents: size of the
training corpus does not affect memory footprint, can process corpora larger than RAM</li>
</ul>
<p>Wall-clock <a class="reference external" href="http://radimrehurek.com/gensim/wiki.html">performance on the English Wikipedia</a>
(2G corpus positions, 3.5M documents, 100K features, 0.54G non-zero entries in the final
bag-of-words matrix), requesting 100 topics:</p>
<table border="1" class="docutils">
<colgroup>
<col width="79%" />
<col width="21%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">algorithm</th>
<th class="head">training time</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>LdaMulticore(workers=1)</td>
<td>2h30m</td>
</tr>
<tr class="row-odd"><td>LdaMulticore(workers=2)</td>
<td>1h24m</td>
</tr>
<tr class="row-even"><td>LdaMulticore(workers=3)</td>
<td>1h6m</td>
</tr>
<tr class="row-odd"><td>old LdaModel()</td>
<td>3h44m</td>
</tr>
<tr class="row-even"><td>simply iterating over input corpus = I/O overhead</td>
<td>20m</td>
</tr>
</tbody>
</table>
<p>(Measured on <a class="reference external" href="http://www.hetzner.de/en/hosting/produkte_rootserver/ex40ssd">this i7 server</a>
with 4 physical cores, so that optimal <cite>workers=3</cite>, one less than the number of cores.)</p>
<p>This module allows both LDA model estimation from a training corpus and inference of topic
distribution on new, unseen documents. The model can also be updated with new documents
for online training.</p>
<p>The core estimation code is based on the <cite>onlineldavb.py</cite> script by M. Hoffman <a class="footnote-reference" href="#id2" id="id1">[1]</a>, see
<strong>Hoffman, Blei, Bach: Online Learning for Latent Dirichlet Allocation, NIPS 2010.</strong></p>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td><a class="reference external" href="http://www.cs.princeton.edu/~mdhoffma">http://www.cs.princeton.edu/~mdhoffma</a></td></tr>
</tbody>
</table>
<dl class="class">
<dt id="gensim.models.ldamulticore.LdaMulticore">
<em class="property">class </em><code class="descclassname">gensim.models.ldamulticore.</code><code class="descname">LdaMulticore</code><span class="sig-paren">(</span><em>corpus=None</em>, <em>num_topics=100</em>, <em>id2word=None</em>, <em>workers=None</em>, <em>chunksize=2000</em>, <em>passes=1</em>, <em>batch=False</em>, <em>alpha='symmetric'</em>, <em>eta=None</em>, <em>decay=0.5</em>, <em>offset=1.0</em>, <em>eval_every=10</em>, <em>iterations=50</em>, <em>gamma_threshold=0.001</em>, <em>random_state=None</em>, <em>minimum_probability=0.01</em>, <em>minimum_phi_value=0.01</em>, <em>per_word_topics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="ldamodel.html#gensim.models.ldamodel.LdaModel" title="gensim.models.ldamodel.LdaModel"><code class="xref py py-class docutils literal"><span class="pre">gensim.models.ldamodel.LdaModel</span></code></a></p>
<p>The constructor estimates Latent Dirichlet Allocation model parameters based
on a training corpus:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span> <span class="o">=</span> <span class="n">LdaMulticore</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>You can then infer topic distributions on new, unseen documents, with</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">doc_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="p">[</span><span class="n">doc_bow</span><span class="p">]</span>
</pre></div>
</div>
<p>The model can be updated (trained) with new documents via</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">other_corpus</span><span class="p">)</span>
</pre></div>
</div>
<p>Model persistency is achieved through its <cite>load</cite>/<cite>save</cite> methods.</p>
<p>If given, start training from the iterable <cite>corpus</cite> straight away. If not given,
the model is left untrained (presumably because you want to call <cite>update()</cite> manually).</p>
<p><cite>num_topics</cite> is the number of requested latent topics to be extracted from
the training corpus.</p>
<p><cite>id2word</cite> is a mapping from word ids (integers) to words (strings). It is
used to determine the vocabulary size, as well as for debugging and topic
printing.</p>
<p><cite>workers</cite> is the number of extra processes to use for parallelization. Uses
all available cores by default: <cite>workers=cpu_count()-1</cite>. <strong>Note</strong>: for
hyper-threaded CPUs, <cite>cpu_count()</cite> returns a useless number – set <cite>workers</cite>
directly to the number of your <strong>real</strong> cores (not hyperthreads) minus one,
for optimal performance.</p>
<p>If <cite>batch</cite> is not set, perform online training by updating the model once
every <cite>workers * chunksize</cite> documents (online training). Otherwise,
run batch LDA, updating model only once at the end of each full corpus pass.</p>
<p><cite>alpha</cite> and <cite>eta</cite> are hyperparameters that affect sparsity of the document-topic
(theta) and topic-word (lambda) distributions. Both default to a symmetric
1.0/num_topics prior.</p>
<p><cite>alpha</cite> can be set to an explicit array = prior of your choice. It also
support special values of ‘asymmetric’ and ‘auto’: the former uses a fixed
normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric
prior directly from your data.</p>
<p><cite>eta</cite> can be a scalar for a symmetric prior over topic/word
distributions, or a matrix of shape num_topics x num_words,
which can be used to impose asymmetric priors over the word
distribution on a per-topic basis. This may be useful if you
want to seed certain topics with particular words by boosting
the priors for those words.</p>
<p>Calculate and log perplexity estimate from the latest mini-batch once every
<cite>eval_every</cite> documents. Set to <cite>None</cite> to disable perplexity estimation (faster),
or to <cite>0</cite> to only evaluate perplexity once, at the end of each corpus pass.</p>
<p><cite>decay</cite> and <cite>offset</cite> parameters are the same as Kappa and Tau_0 in
Hoffman et al, respectively.</p>
<p><cite>random_state</cite> can be a numpy.random.RandomState object or the seed for one</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span> <span class="o">=</span> <span class="n">LdaMulticore</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">id2word</span><span class="o">=</span><span class="n">id2word</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># train model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">lda</span><span class="p">[</span><span class="n">doc_bow</span><span class="p">])</span> <span class="c1"># get topic probability distribution for a document</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">corpus2</span><span class="p">)</span> <span class="c1"># update the LDA model with additional documents</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">lda</span><span class="p">[</span><span class="n">doc_bow</span><span class="p">])</span>
</pre></div>
</div>
<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.bound">
<code class="descname">bound</code><span class="sig-paren">(</span><em>corpus</em>, <em>gamma=None</em>, <em>subsample_ratio=1.0</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.bound" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate the variational bound of documents from <cite>corpus</cite>:
E_q[log p(corpus)] - E_q[log q(corpus)]</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>corpus</strong> – documents to infer variational bounds from.</li>
<li><strong>gamma</strong> – the variational parameters on topic weights for each <cite>corpus</cite>
document (=2d matrix=what comes out of <cite>inference()</cite>).
If not supplied, will be inferred from the model.</li>
<li><strong>subsample_ratio</strong> (<em>float</em>) – If <cite>corpus</cite> is a sample of the whole corpus,
pass this to inform on what proportion of the corpus it represents.
This is used as a multiplicative factor to scale the likelihood
appropriately.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The variational bound score calculated.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.clear">
<code class="descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear model state (free up some memory). Used in the distributed algo.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.diff">
<code class="descname">diff</code><span class="sig-paren">(</span><em>other</em>, <em>distance='kullback_leibler'</em>, <em>num_words=100</em>, <em>n_ann_terms=10</em>, <em>diagonal=False</em>, <em>annotation=True</em>, <em>normed=True</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.diff" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate difference topic2topic between two Lda models
<cite>other</cite> instances of <cite>LdaMulticore</cite> or <cite>LdaModel</cite>
<cite>distance</cite> is function that will be applied to calculate difference between any topic pair.
Available values: <cite>kullback_leibler</cite>, <cite>hellinger</cite>, <cite>jaccard</cite> and <cite>jensen_shannon</cite>
<cite>num_words</cite> is quantity of most relevant words that used if distance == <cite>jaccard</cite> (also used for annotation)
<cite>n_ann_terms</cite> is max quantity of words in intersection/symmetric difference between topics (used for annotation)
<cite>diagonal</cite> set to True if the difference is required only between the identical topic no.s (returns diagonal of diff matrix)
<cite>annotation</cite> whether the intersection or difference of words between two topics should be returned
Returns a matrix Z with shape (m1.num_topics, m2.num_topics), where Z[i][j] - difference between topic_i and topic_j
and matrix annotation (if True) with shape (m1.num_topics, m2.num_topics, 2, None),
where:</p>
<blockquote>
<div>annotation[i][j] = [[<cite>int_1</cite>, <cite>int_2</cite>, …], [<cite>diff_1</cite>, <cite>diff_2</cite>, …]] and
<cite>int_k</cite> is word from intersection of <cite>topic_i</cite> and <cite>topic_j</cite> and
<cite>diff_l</cite> is word from symmetric difference of <cite>topic_i</cite> and <cite>topic_j</cite>
<cite>normed</cite> is a flag. If <cite>true</cite>, matrix Z will be normalized</div></blockquote>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span> <span class="o">=</span> <span class="n">LdaMulticore</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path_1</span><span class="p">),</span> <span class="n">LdaMulticore</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path_2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mdiff</span><span class="p">,</span> <span class="n">annotation</span> <span class="o">=</span> <span class="n">m1</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">m2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">mdiff</span><span class="p">)</span> <span class="c1"># get matrix with difference for each topic pair from `m1` and `m2`</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">annotation</span><span class="p">)</span> <span class="c1"># get array with positive/negative words for each topic pair from `m1` and `m2`</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.do_estep">
<code class="descname">do_estep</code><span class="sig-paren">(</span><em>chunk</em>, <em>state=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.do_estep" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform inference on a chunk of documents, and accumulate the collected
sufficient statistics in <cite>state</cite> (or <cite>self.state</cite> if None).</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.do_mstep">
<code class="descname">do_mstep</code><span class="sig-paren">(</span><em>rho</em>, <em>other</em>, <em>extra_pass=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.do_mstep" title="Permalink to this definition">¶</a></dt>
<dd><p>M step: use linear interpolation between the existing topics and
collected sufficient statistics in <cite>other</cite> to update the topics.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.get_document_topics">
<code class="descname">get_document_topics</code><span class="sig-paren">(</span><em>bow</em>, <em>minimum_probability=None</em>, <em>minimum_phi_value=None</em>, <em>per_word_topics=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.get_document_topics" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>bow</strong> (<em>list</em>) – Bag-of-words representation of the document to get topics for.</li>
<li><strong>minimum_probability</strong> (<em>float</em>) – Ignore topics with probability below this value
(None by default). If set to None, a value of 1e-8 is used to prevent 0s.</li>
<li><strong>per_word_topics</strong> (<em>bool</em>) – If True, also returns a list of topics, sorted in
descending order of most likely topics for that word. It also returns a list
of word_ids and each words corresponding topics’ phi_values, multiplied by
feature length (i.e, word count).</li>
<li><strong>minimum_phi_value</strong> (<em>float</em>) – if <cite>per_word_topics</cite> is True, this represents a lower
bound on the term probabilities that are included (None by default). If set
to None, a value of 1e-8 is used to prevent 0s.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">topic distribution for the given document <cite>bow</cite>, as a list of
<cite>(topic_id, topic_probability)</cite> 2-tuples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.get_term_topics">
<code class="descname">get_term_topics</code><span class="sig-paren">(</span><em>word_id</em>, <em>minimum_probability=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.get_term_topics" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>word_id</strong> (<em>int</em>) – ID of the word to get topic probabilities for.</li>
<li><strong>minimum_probability</strong> (<em>float</em>) – Only include topic probabilities above this
value (None by default). If set to None, use 1e-8 to prevent including 0s.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The most likely topics for the given word. Each topic is represented
as a tuple of <cite>(topic_id, term_probability)</cite>.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.get_topic_terms">
<code class="descname">get_topic_terms</code><span class="sig-paren">(</span><em>topicid</em>, <em>topn=10</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.get_topic_terms" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>topn</strong> (<em>int</em>) – Only return 2-tuples for the topn most probable words
(ignore the rest).</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><cite>(word_id, probability)</cite> 2-tuples for the most probable words
in topic with id <cite>topicid</cite>.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.get_topics">
<code class="descname">get_topics</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.get_topics" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><cite>num_topics</cite> x <cite>vocabulary_size</cite> array of floats which represents
the term topic matrix learned during inference.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">np.ndarray</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.inference">
<code class="descname">inference</code><span class="sig-paren">(</span><em>chunk</em>, <em>collect_sstats=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a chunk of sparse document vectors, estimate gamma (parameters
controlling the topic weights) for each document in the chunk.</p>
<p>This function does not modify the model (=is read-only aka const). The
whole input chunk of document is assumed to fit in RAM; chunking of a
large corpus must be done earlier in the pipeline.</p>
<p>If <cite>collect_sstats</cite> is True, also collect sufficient statistics needed
to update the model’s topic-word distributions, and return a 2-tuple
<cite>(gamma, sstats)</cite>. Otherwise, return <cite>(gamma, None)</cite>. <cite>gamma</cite> is of shape
<cite>len(chunk) x self.num_topics</cite>.</p>
<p>Avoids computing the <cite>phi</cite> variational parameter directly using the
optimization presented in <strong>Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001</strong>.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.init_dir_prior">
<code class="descname">init_dir_prior</code><span class="sig-paren">(</span><em>prior</em>, <em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.init_dir_prior" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>fname</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a previously saved object from file (also see <cite>save</cite>).</p>
<p>Large arrays can be memmap’ed back as read-only (shared memory) by setting <cite>mmap=’r’</cite>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">LdaModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="n">mmap</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.log_perplexity">
<code class="descname">log_perplexity</code><span class="sig-paren">(</span><em>chunk</em>, <em>total_docs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.log_perplexity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate and return per-word likelihood bound, using the <cite>chunk</cite> of
documents as evaluation corpus. Also output the calculated statistics. incl.
perplexity=2^(-bound), to log at INFO level.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.print_topic">
<code class="descname">print_topic</code><span class="sig-paren">(</span><em>topicno</em>, <em>topn=10</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.print_topic" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a single topic as a formatted string. See <cite>show_topic()</cite> for parameters.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lsimodel</span><span class="o">.</span><span class="n">print_topic</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">&#39;-0.340 * &quot;category&quot; + 0.298 * &quot;$M$&quot; + 0.183 * &quot;algebra&quot; + -0.174 * &quot;functor&quot; + -0.168 * &quot;operator&quot;&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.print_topics">
<code class="descname">print_topics</code><span class="sig-paren">(</span><em>num_topics=20</em>, <em>num_words=10</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.print_topics" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <cite>show_topics()</cite> that prints the <cite>num_words</cite> most
probable words for <cite>topics</cite> number of topics to log.
Set <cite>topics=-1</cite> to print all topics.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>fname</em>, <em>ignore=('state'</em>, <em>'dispatcher')</em>, <em>separately=None</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the model to file.</p>
<p>Large internal arrays may be stored into separate files, with <cite>fname</cite> as prefix.</p>
<p><cite>separately</cite> can be used to define which arrays should be stored in separate files.</p>
<p><cite>ignore</cite> parameter can be used to define which variables should be ignored, i.e. left
out from the pickled lda model. By default the internal <cite>state</cite> is ignored as it uses
its own serialisation not the one provided by <cite>LdaModel</cite>. The <cite>state</cite> and <cite>dispatcher</cite>
will be added to any ignore parameter defined.</p>
<p>Note: do not save as a compressed file if you intend to load the file back with <cite>mmap</cite>.</p>
<p>Note: If you intend to use models across Python 2/3 versions there are a few things to
keep in mind:</p>
<blockquote>
<div><ol class="arabic simple">
<li>The pickled Python dictionaries will not work across Python versions</li>
<li>The <cite>save</cite> method does not automatically save all np arrays using np, only
those ones that exceed <cite>sep_limit</cite> set in <cite>gensim.utils.SaveLoad.save</cite>. The main
concern here is the <cite>alpha</cite> array if for instance using <cite>alpha=’auto’</cite>.</li>
</ol>
</div></blockquote>
<p>Please refer to the wiki recipes section (<a class="reference external" href="https://github.com/piskvorky/gensim/wiki/Recipes-&amp;-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2">https://github.com/piskvorky/gensim/wiki/Recipes-&amp;-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2</a>)
for an example on how to work around these issues.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.show_topic">
<code class="descname">show_topic</code><span class="sig-paren">(</span><em>topicid</em>, <em>topn=10</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.show_topic" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>topn</strong> (<em>int</em>) – Only return 2-tuples for the topn most probable words
(ignore the rest).</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">of <cite>(word, probability)</cite> 2-tuples for the most probable
words in topic <cite>topicid</cite>.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.show_topics">
<code class="descname">show_topics</code><span class="sig-paren">(</span><em>num_topics=10</em>, <em>num_words=10</em>, <em>log=False</em>, <em>formatted=True</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.show_topics" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_topics</strong> (<em>int</em>) – show results for first <cite>num_topics</cite> topics.
Unlike LSA, there is no natural ordering between the topics in LDA.
The returned <cite>num_topics &lt;= self.num_topics</cite> subset of all topics is
therefore arbitrary and may change between two LDA training runs.</li>
<li><strong>num_words</strong> (<em>int</em>) – include top <cite>num_words</cite> with highest probabilities in topic.</li>
<li><strong>log</strong> (<em>bool</em>) – If True, log output in addition to returning it.</li>
<li><strong>formatted</strong> (<em>bool</em>) – If True, format topics as strings, otherwise return them as
<cite>(word, probability)</cite> 2-tuples.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><cite>num_words</cite> most significant words for <cite>num_topics</cite> number of topics
(10 words for top 10 topics, by default).</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.sync_state">
<code class="descname">sync_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.sync_state" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.top_topics">
<code class="descname">top_topics</code><span class="sig-paren">(</span><em>corpus=None</em>, <em>texts=None</em>, <em>dictionary=None</em>, <em>window_size=None</em>, <em>coherence='u_mass'</em>, <em>topn=20</em>, <em>processes=-1</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.top_topics" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the coherence for each topic; default is Umass coherence.</p>
<p>See the <code class="xref py py-class docutils literal"><span class="pre">gensim.models.CoherenceModel</span></code> constructor for more info on the
parameters and the different coherence metrics.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">tuples with <cite>(topic_repr, coherence_score)</cite>, where <cite>topic_repr</cite> is a list
of representations of the <cite>topn</cite> terms for the topic. The terms are represented
as tuples of <cite>(membership_in_topic, token)</cite>. The <cite>coherence_score</cite> is a float.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">list</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>corpus</em>, <em>chunks_as_numpy=False</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Train the model with new documents, by EM-iterating over <cite>corpus</cite> until
the topics converge (or until the maximum number of allowed iterations
is reached). <cite>corpus</cite> must be an iterable (repeatable stream of documents),</p>
<p>The E-step is distributed into the several processes.</p>
<p>This update also supports updating an already trained model (<cite>self</cite>)
with new documents from <cite>corpus</cite>; the two models are then merged in
proportion to the number of old vs. new documents. This feature is still
experimental for non-stationary input streams.</p>
<p>For stationary input (no topic drift in new documents), on the other hand,
this equals the online update of Hoffman et al. and is guaranteed to
converge for any <cite>decay</cite> in (0.5, 1.0&gt;.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.update_alpha">
<code class="descname">update_alpha</code><span class="sig-paren">(</span><em>gammat</em>, <em>rho</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.update_alpha" title="Permalink to this definition">¶</a></dt>
<dd><p>Update parameters for the Dirichlet prior on the per-document
topic weights <cite>alpha</cite> given the last <cite>gammat</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="gensim.models.ldamulticore.LdaMulticore.update_eta">
<code class="descname">update_eta</code><span class="sig-paren">(</span><em>lambdat</em>, <em>rho</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.LdaMulticore.update_eta" title="Permalink to this definition">¶</a></dt>
<dd><p>Update parameters for the Dirichlet prior on the per-topic
word weights <cite>eta</cite> given the last <cite>lambdat</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="gensim.models.ldamulticore.worker_e_step">
<code class="descclassname">gensim.models.ldamulticore.</code><code class="descname">worker_e_step</code><span class="sig-paren">(</span><em>input_queue</em>, <em>result_queue</em><span class="sig-paren">)</span><a class="headerlink" href="#gensim.models.ldamulticore.worker_e_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform E-step for each (chunk_no, chunk, model) 3-tuple from the
input queue, placing the resulting state into the result queue.</p>
</dd></dl>

</div>


          </div>
        </div>
      

      <div class="clearer"></div>
    </div>
    

    
    <div id="footer">
      <div id="footerwrapper">
        <div id="footerleft">
          <img src="../_static/images/logo-gensim.png" class="smallerlogo" alt="smaller gensim logo" />
          <a href="../index.html"><img src="../_static/images/gensim-footer.png" alt="gensim footer image" title="Gensim home" /></a>

          <div class="copyright">
            &copy; Copyright 2009-now, <a href="mailto:radimrehurek@seznam.cz" style="color:white"> Radim Řehůřek</a>
            <br />
              Last updated on Nov 06, 2017.
          </div>
        </div>

        <div id="footermiddleright">
          <div id="footermiddle">
            <ul class="navigation">
              <li><a href="../index.html">
                Home
              </a></li>
              <li>|</li>
              <li><a href="../tutorial.html">
                Tutorials
              </a></li>
              <li>|</li>
              <li><a href="../install.html">
                Install
              </a></li>
              <li>|</li>
              <li><a href="../support.html">
                Support
              </a></li>
              <li>|</li>
              <li><a href="../apiref.html">
                API
              </a></li>
              <li>|</li>
              <li><a href="../about.html">
                About
              </a></li>
            </ul>

            <div class="tweetodsazeni">
              <div class="tweet">
                <a href="https://twitter.com/radimrehurek" target="_blank" style="color: white">Tweet @RadimRehurek</a>
              </div>
            </div>

          </div>

          <div id="footerright">
            <div class="footernadpis">
              Support:
            </div>
            <div class="googlegroupsodsazeni">
              <a href="https://groups.google.com/group/gensim" class="googlegroups">
                Stay informed via gensim mailing list:
              </a>

              <form action="http://groups.google.com/group/gensim/boxsubscribe">
                <input type="text" name="email" placeholder="your@email.com" size="28" />
                <input type="submit" name="sub" value="Subscribe" />
              </form>

            </div>

            <div class="addthis_toolbox addthis_default_style addthis_32x32_style"
                addthis:title="#gensim"
                addthis:description="Efficient Topic Modelling in Python"
                style="margin:20px 0 0 0">
              <a class="addthis_button_preferred_1"></a>
              <a class="addthis_button_preferred_2"></a>
              <a class="addthis_button_preferred_3"></a>
              <a class="addthis_button_preferred_4"></a>
              <a class="addthis_button_compact"></a>
              <a class="addthis_counter addthis_bubble_style"></a>
            </div>
          </div>

        </div>
      </div>
    </div>
    

    <script type="text/javascript">
      (function() {
      var at = document.createElement('script'); at.type = 'text/javascript'; at.async = true;
      at.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 's7.addthis.com/js/250/addthis_widget.js#pubid=ra-4d738b9b1d31ccbd';
      var sat = document.getElementsByTagName('script')[0]; sat.parentNode.insertBefore(at, sat);
      })();
    </script>

  </body>
</html>