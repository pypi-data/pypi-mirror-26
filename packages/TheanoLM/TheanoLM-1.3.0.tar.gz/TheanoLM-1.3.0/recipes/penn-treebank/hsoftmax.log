/l/senarvi/theanolm-recipes/penn-treebank/nnlm.vocab
THEANO_FLAGS=floatX=float32,device=gpu,nvcc.fastmath=True
Reading vocabulary from /l/senarvi/theanolm-recipes/penn-treebank/nnlm.vocab.
Number of words in vocabulary: 10001
Number of word classes: 10001
2017-01-12 01:04:33,979 train: TRAINING OPTIONS
2017-01-12 01:04:33,979 train: patience: 0
2017-01-12 01:04:33,979 train: batch_size: 32
2017-01-12 01:04:33,979 train: stopping_criterion: no-improvement
2017-01-12 01:04:33,979 train: max_annealing_count: 0
2017-01-12 01:04:33,979 train: sequence_length: 25
2017-01-12 01:04:33,979 train: min_epochs: 1
2017-01-12 01:04:33,979 train: validation_frequency: 1
2017-01-12 01:04:33,979 train: max_epochs: 15
2017-01-12 01:04:33,979 train: OPTIMIZATION OPTIONS
2017-01-12 01:04:33,979 train: ignore_unk: False
2017-01-12 01:04:33,979 train: method: adagrad
2017-01-12 01:04:33,979 train: noise_sharing: None
2017-01-12 01:04:33,980 train: weights: [ 1.]
2017-01-12 01:04:33,980 train: max_gradient_norm: 5.0
2017-01-12 01:04:33,980 train: sqr_gradient_decay_rate: 0.999
2017-01-12 01:04:33,980 train: gradient_decay_rate: 0.9
2017-01-12 01:04:33,980 train: epsilon: 1e-06
2017-01-12 01:04:33,980 train: num_noise_samples: 1
2017-01-12 01:04:33,980 train: learning_rate: 1.0
2017-01-12 01:04:33,980 train: unk_penalty: None
2017-01-12 01:04:33,980 train: momentum: 0.9
2017-01-12 01:04:33,980 train: cost_function: cross-entropy
Creating trainer.
Computing unigram probabilities and the number of mini-batches in training data.
2017-01-12 01:04:35,101 __init__: One epoch of training data contains 1778 mini-batch updates.
2017-01-12 01:04:35,101 __init__: Class unigram probabilities are in the range [0.00000103, 0.05232915].
2017-01-12 01:04:35,101 __init__: Finding sentence start positions in /teamwork/t40511_asr/c/penn-treebank-project/ptb.train.txt.
2017-01-12 01:04:35,124 _reset: Generating a random order of input lines.
Building neural network.
2017-01-12 01:04:35,131 __init__: Creating layers.
2017-01-12 01:04:35,131 __init__: - NetworkInput name=word_input inputs=[] size=10001, devices=[]
2017-01-12 01:04:35,131 __init__: - ProjectionLayer name=projection_layer inputs=[word_input] size=100, devices=[None]
2017-01-12 01:04:35,213 add:      * layers/projection_layer/W size=1000100 type=float32 device=None
2017-01-12 01:04:35,213 __init__: - LSTMLayer name=hidden_layer inputs=[projection_layer] size=256, devices=[None]
2017-01-12 01:04:35,222 add:      * layers/hidden_layer/layer_input/W size=102400 type=float32 device=None
2017-01-12 01:04:35,478 add:      * layers/hidden_layer/step_input/W size=262144 type=float32 device=None
2017-01-12 01:04:35,478 add:      * layers/hidden_layer/layer_input/b size=1024 type=float32 device=None
2017-01-12 01:04:35,478 __init__: - HSoftmaxLayer name=output_layer inputs=[hidden_layer] size=10001, devices=[None]
2017-01-12 01:04:35,478 __init__:   level1_size=101 level2_size=100
2017-01-12 01:04:35,481 add:      * layers/output_layer/input/W size=25856 type=float32 device=None
2017-01-12 01:04:35,482 add:      * layers/output_layer/input/b size=101 type=float32 device=None
2017-01-12 01:04:35,700 add:      * layers/output_layer/level1/W size=2585600 type=float32 device=None
2017-01-12 01:04:35,701 add:      * layers/output_layer/level1/b size=10100 type=float32 device=None
2017-01-12 01:04:35,701 __init__: Total number of parameters: 3987325
Compiling optimization function.
2017-01-12 01:04:35,804 add:      * layers/hidden_layer/layer_input/b_gradient size=1024 type=float32 device=None
2017-01-12 01:04:35,804 add:      * layers/hidden_layer/layer_input/b_sum_sqr_gradient size=1024 type=float32 device=None
2017-01-12 01:04:35,805 add:      * layers/output_layer/input/W_gradient size=25856 type=float32 device=None
2017-01-12 01:04:35,805 add:      * layers/output_layer/input/W_sum_sqr_gradient size=25856 type=float32 device=None
2017-01-12 01:04:35,805 add:      * layers/output_layer/input/b_gradient size=101 type=float32 device=None
2017-01-12 01:04:35,805 add:      * layers/output_layer/input/b_sum_sqr_gradient size=101 type=float32 device=None
2017-01-12 01:04:35,810 add:      * layers/output_layer/level1/W_gradient size=2585600 type=float32 device=None
2017-01-12 01:04:35,815 add:      * layers/output_layer/level1/W_sum_sqr_gradient size=2585600 type=float32 device=None
2017-01-12 01:04:35,815 add:      * layers/output_layer/level1/b_gradient size=10100 type=float32 device=None
2017-01-12 01:04:35,815 add:      * layers/output_layer/level1/b_sum_sqr_gradient size=10100 type=float32 device=None
2017-01-12 01:04:35,817 add:      * layers/projection_layer/W_gradient size=1000100 type=float32 device=None
2017-01-12 01:04:35,819 add:      * layers/projection_layer/W_sum_sqr_gradient size=1000100 type=float32 device=None
2017-01-12 01:04:35,820 add:      * layers/hidden_layer/layer_input/W_gradient size=102400 type=float32 device=None
2017-01-12 01:04:35,820 add:      * layers/hidden_layer/layer_input/W_sum_sqr_gradient size=102400 type=float32 device=None
2017-01-12 01:04:35,821 add:      * layers/hidden_layer/step_input/W_gradient size=262144 type=float32 device=None
2017-01-12 01:04:35,822 add:      * layers/hidden_layer/step_input/W_sum_sqr_gradient size=262144 type=float32 device=None
Building text scorer for cross-validation.
Validation text: /teamwork/t40511_asr/c/penn-treebank-project/ptb.valid.txt
Training neural network.
2017-01-12 01:05:01,453 _log_update: [200] (11.2 %) of epoch 1 -- lr = 1, cost = 6.01, duration = 7.6 ms
2017-01-12 01:05:16,698 _log_update: [400] (22.5 %) of epoch 1 -- lr = 1, cost = 5.88, duration = 7.5 ms
2017-01-12 01:05:31,936 _log_update: [600] (33.7 %) of epoch 1 -- lr = 1, cost = 5.39, duration = 7.7 ms
2017-01-12 01:05:47,185 _log_update: [800] (45.0 %) of epoch 1 -- lr = 1, cost = 5.36, duration = 7.5 ms
2017-01-12 01:06:02,406 _log_update: [1000] (56.2 %) of epoch 1 -- lr = 1, cost = 5.56, duration = 7.6 ms
2017-01-12 01:06:17,628 _log_update: [1200] (67.5 %) of epoch 1 -- lr = 1, cost = 5.28, duration = 7.6 ms
2017-01-12 01:06:32,845 _log_update: [1400] (78.7 %) of epoch 1 -- lr = 1, cost = 5.55, duration = 7.5 ms
2017-01-12 01:06:48,068 _log_update: [1600] (90.0 %) of epoch 1 -- lr = 1, cost = 5.08, duration = 7.6 ms
2017-01-12 01:07:03,446 _validate: [1772] First validation sample, perplexity 168.64.
2017-01-12 01:07:10,486 _validate: [1775] Center of validation, perplexity 168.62.
2017-01-12 01:07:17,648 _validate: [1778] Last validation sample, perplexity 168.47.
2017-01-12 01:07:17,685 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2017-01-12 01:07:17,686 _log_validation: [1778] Validation set cost history: [168.6]
2017-01-12 01:07:17,687 _reset: Generating a random order of input lines.
Finished training epoch 1 in 0 hours 2.5 minutes. Best validation perplexity 168.57.
2017-01-12 01:07:19,367 _log_update: [22] (1.2 %) of epoch 2 -- lr = 1, cost = 4.67, duration = 7.5 ms
2017-01-12 01:07:34,588 _log_update: [222] (12.5 %) of epoch 2 -- lr = 1, cost = 4.81, duration = 7.5 ms
2017-01-12 01:07:49,795 _log_update: [422] (23.7 %) of epoch 2 -- lr = 1, cost = 5.19, duration = 7.5 ms
2017-01-12 01:08:05,007 _log_update: [622] (35.0 %) of epoch 2 -- lr = 1, cost = 4.94, duration = 7.6 ms
2017-01-12 01:08:20,234 _log_update: [822] (46.2 %) of epoch 2 -- lr = 1, cost = 4.92, duration = 7.6 ms
2017-01-12 01:08:35,459 _log_update: [1022] (57.5 %) of epoch 2 -- lr = 1, cost = 4.77, duration = 7.5 ms
2017-01-12 01:08:50,686 _log_update: [1222] (68.7 %) of epoch 2 -- lr = 1, cost = 4.99, duration = 7.6 ms
2017-01-12 01:09:05,906 _log_update: [1422] (80.0 %) of epoch 2 -- lr = 1, cost = 4.44, duration = 7.5 ms
2017-01-12 01:09:21,145 _log_update: [1622] (91.2 %) of epoch 2 -- lr = 1, cost = 4.98, duration = 7.5 ms
2017-01-12 01:09:34,820 _validate: [1772] First validation sample, perplexity 139.58.
2017-01-12 01:09:41,832 _validate: [1775] Center of validation, perplexity 139.18.
2017-01-12 01:09:48,878 _validate: [1778] Last validation sample, perplexity 139.03.
2017-01-12 01:09:48,902 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2017-01-12 01:09:48,902 _log_validation: [1778] Validation set cost history: 168.6 [139.2]
2017-01-12 01:09:48,904 _reset: Generating a random order of input lines.
Finished training epoch 2 in 0 hours 2.5 minutes. Best validation perplexity 139.18.
2017-01-12 01:09:52,259 _log_update: [44] (2.5 %) of epoch 3 -- lr = 1, cost = 4.22, duration = 7.5 ms
2017-01-12 01:10:07,465 _log_update: [244] (13.7 %) of epoch 3 -- lr = 1, cost = 4.44, duration = 7.6 ms
2017-01-12 01:10:22,673 _log_update: [444] (25.0 %) of epoch 3 -- lr = 1, cost = 4.68, duration = 7.5 ms
2017-01-12 01:10:37,890 _log_update: [644] (36.2 %) of epoch 3 -- lr = 1, cost = 4.29, duration = 7.6 ms
2017-01-12 01:10:53,141 _log_update: [844] (47.5 %) of epoch 3 -- lr = 1, cost = 4.67, duration = 7.5 ms
2017-01-12 01:11:08,345 _log_update: [1044] (58.7 %) of epoch 3 -- lr = 1, cost = 4.59, duration = 7.5 ms
2017-01-12 01:11:23,564 _log_update: [1244] (70.0 %) of epoch 3 -- lr = 1, cost = 4.63, duration = 7.5 ms
2017-01-12 01:11:38,769 _log_update: [1444] (81.2 %) of epoch 3 -- lr = 1, cost = 4.09, duration = 7.5 ms
2017-01-12 01:11:53,985 _log_update: [1644] (92.5 %) of epoch 3 -- lr = 1, cost = 4.93, duration = 7.5 ms
2017-01-12 01:12:05,988 _validate: [1772] First validation sample, perplexity 132.85.
2017-01-12 01:12:13,011 _validate: [1775] Center of validation, perplexity 132.99.
2017-01-12 01:12:20,043 _validate: [1778] Last validation sample, perplexity 132.88.
2017-01-12 01:12:20,065 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2017-01-12 01:12:20,065 _log_validation: [1778] Validation set cost history: 168.6 139.2 [132.9]
2017-01-12 01:12:20,067 _reset: Generating a random order of input lines.
Finished training epoch 3 in 0 hours 2.5 minutes. Best validation perplexity 132.90.
2017-01-12 01:12:25,089 _log_update: [66] (3.7 %) of epoch 4 -- lr = 1, cost = 4.05, duration = 7.5 ms
2017-01-12 01:12:40,296 _log_update: [266] (15.0 %) of epoch 4 -- lr = 1, cost = 4.17, duration = 7.5 ms
2017-01-12 01:12:55,490 _log_update: [466] (26.2 %) of epoch 4 -- lr = 1, cost = 4.40, duration = 7.5 ms
2017-01-12 01:13:10,686 _log_update: [666] (37.5 %) of epoch 4 -- lr = 1, cost = 4.59, duration = 7.5 ms
2017-01-12 01:13:25,886 _log_update: [866] (48.7 %) of epoch 4 -- lr = 1, cost = 4.16, duration = 7.5 ms
2017-01-12 01:13:41,077 _log_update: [1066] (60.0 %) of epoch 4 -- lr = 1, cost = 3.93, duration = 7.5 ms
2017-01-12 01:13:56,265 _log_update: [1266] (71.2 %) of epoch 4 -- lr = 1, cost = 4.48, duration = 7.5 ms
2017-01-12 01:14:11,458 _log_update: [1466] (82.5 %) of epoch 4 -- lr = 1, cost = 4.57, duration = 7.5 ms
2017-01-12 01:14:26,656 _log_update: [1666] (93.7 %) of epoch 4 -- lr = 1, cost = 4.59, duration = 7.5 ms
2017-01-12 01:14:36,964 _validate: [1772] First validation sample, perplexity 133.41.
2017-01-12 01:14:43,964 _validate: [1775] Center of validation, perplexity 133.42.
2017-01-12 01:14:51,011 _validate: [1778] Last validation sample, perplexity 133.49.
2017-01-12 01:14:51,011 _log_validation: [1778] Validation set cost history: 168.6 139.2 [132.9] 133.4
2017-01-12 01:14:51,013 set_state: layers/projection_layer/W <- array(10001, 100)
2017-01-12 01:14:51,013 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2017-01-12 01:14:51,014 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2017-01-12 01:14:51,014 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2017-01-12 01:14:51,015 set_state: layers/output_layer/input/W <- array(256, 101)
2017-01-12 01:14:51,015 set_state: layers/output_layer/input/b <- array(101,)
2017-01-12 01:14:51,015 set_state: layers/output_layer/level1/b <- array(101, 100)
2017-01-12 01:14:51,018 set_state: layers/output_layer/level1/W <- array(101, 256, 100)
2017-01-12 01:14:51,019 _reset_state: [1775] (99.83 %) of epoch 3
2017-01-12 01:14:51,020 _log_validation: [1775] Validation set cost history: 168.6 139.2 [132.9]
2017-01-12 01:14:51,020 set_state: Restored iterator to line 41998 of 42068.
2017-01-12 01:14:51,021 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(101,)
2017-01-12 01:14:51,024 set_state: layers/output_layer/level1/W_gradient <- array(101, 256, 100)
2017-01-12 01:14:51,024 set_state: layers/output_layer/input/W_gradient <- array(256, 101)
2017-01-12 01:14:51,025 set_state: layers/hidden_layer/layer_input/W_sum_sqr_gradient <- array(100, 1024)
2017-01-12 01:14:51,028 set_state: layers/output_layer/level1/W_sum_sqr_gradient <- array(101, 256, 100)
2017-01-12 01:14:51,029 set_state: layers/projection_layer/W_gradient <- array(10001, 100)
2017-01-12 01:14:51,030 set_state: layers/hidden_layer/step_input/W_gradient <- array(256, 1024)
2017-01-12 01:14:51,030 set_state: layers/output_layer/level1/b_sum_sqr_gradient <- array(101, 100)
2017-01-12 01:14:51,031 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 101)
2017-01-12 01:14:51,031 set_state: layers/hidden_layer/step_input/W_sum_sqr_gradient <- array(256, 1024)
2017-01-12 01:14:51,032 set_state: layers/output_layer/input/b_gradient <- array(101,)
2017-01-12 01:14:51,032 set_state: layers/hidden_layer/layer_input/b_sum_sqr_gradient <- array(1024,)
2017-01-12 01:14:51,033 set_state: layers/projection_layer/W_sum_sqr_gradient <- array(10001, 100)
2017-01-12 01:14:51,033 set_state: layers/output_layer/level1/b_gradient <- array(101, 100)
2017-01-12 01:14:51,034 set_state: layers/hidden_layer/layer_input/b_gradient <- array(1024,)
2017-01-12 01:14:51,034 set_state: layers/hidden_layer/layer_input/W_gradient <- array(100, 1024)
Model performance stopped improving. Decreasing learning rate from 1.0 to 0.5 and resetting state to 100 % of epoch 3.
2017-01-12 01:14:51,036 _reset: Generating a random order of input lines.
Finished training epoch 3 in 0 hours 2.5 minutes. Best validation perplexity 132.90.
2017-01-12 01:14:57,743 _log_update: [88] (4.9 %) of epoch 4 -- lr = 0.5, cost = 4.63, duration = 7.5 ms
2017-01-12 01:15:12,945 _log_update: [288] (16.2 %) of epoch 4 -- lr = 0.5, cost = 4.16, duration = 7.6 ms
2017-01-12 01:15:28,134 _log_update: [488] (27.4 %) of epoch 4 -- lr = 0.5, cost = 4.35, duration = 7.5 ms
2017-01-12 01:15:43,330 _log_update: [688] (38.7 %) of epoch 4 -- lr = 0.5, cost = 4.06, duration = 7.5 ms
2017-01-12 01:15:58,522 _log_update: [888] (49.9 %) of epoch 4 -- lr = 0.5, cost = 4.12, duration = 7.5 ms
2017-01-12 01:16:13,722 _log_update: [1088] (61.2 %) of epoch 4 -- lr = 0.5, cost = 4.41, duration = 7.6 ms
2017-01-12 01:16:28,920 _log_update: [1288] (72.4 %) of epoch 4 -- lr = 0.5, cost = 4.22, duration = 7.6 ms
2017-01-12 01:16:44,111 _log_update: [1488] (83.7 %) of epoch 4 -- lr = 0.5, cost = 4.50, duration = 7.5 ms
2017-01-12 01:16:59,302 _log_update: [1688] (94.9 %) of epoch 4 -- lr = 0.5, cost = 4.05, duration = 7.5 ms
2017-01-12 01:17:07,939 _validate: [1772] First validation sample, perplexity 131.41.
2017-01-12 01:17:14,915 _validate: [1775] Center of validation, perplexity 131.30.
2017-01-12 01:17:21,930 _validate: [1778] Last validation sample, perplexity 131.26.
2017-01-12 01:17:21,951 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2017-01-12 01:17:21,951 _log_validation: [1778] Validation set cost history: 168.6 139.2 132.9 [131.3]
2017-01-12 01:17:21,953 _reset: Generating a random order of input lines.
Finished training epoch 4 in 0 hours 2.5 minutes. Best validation perplexity 131.31.
2017-01-12 01:17:30,315 _log_update: [110] (6.2 %) of epoch 5 -- lr = 0.5, cost = 4.02, duration = 7.5 ms
2017-01-12 01:17:45,507 _log_update: [310] (17.4 %) of epoch 5 -- lr = 0.5, cost = 4.00, duration = 7.5 ms
2017-01-12 01:18:00,696 _log_update: [510] (28.7 %) of epoch 5 -- lr = 0.5, cost = 4.04, duration = 7.5 ms
2017-01-12 01:18:15,892 _log_update: [710] (39.9 %) of epoch 5 -- lr = 0.5, cost = 4.16, duration = 7.6 ms
2017-01-12 01:18:31,090 _log_update: [910] (51.2 %) of epoch 5 -- lr = 0.5, cost = 4.27, duration = 7.5 ms
2017-01-12 01:18:46,288 _log_update: [1110] (62.4 %) of epoch 5 -- lr = 0.5, cost = 4.20, duration = 7.5 ms
2017-01-12 01:19:01,480 _log_update: [1310] (73.7 %) of epoch 5 -- lr = 0.5, cost = 4.02, duration = 7.5 ms
2017-01-12 01:19:16,686 _log_update: [1510] (84.9 %) of epoch 5 -- lr = 0.5, cost = 4.33, duration = 7.5 ms
2017-01-12 01:19:31,880 _log_update: [1710] (96.2 %) of epoch 5 -- lr = 0.5, cost = 4.14, duration = 7.6 ms
2017-01-12 01:19:38,847 _validate: [1772] First validation sample, perplexity 135.28.
2017-01-12 01:19:45,820 _validate: [1775] Center of validation, perplexity 135.48.
2017-01-12 01:19:52,832 _validate: [1778] Last validation sample, perplexity 135.24.
2017-01-12 01:19:52,832 _log_validation: [1778] Validation set cost history: 168.6 139.2 132.9 [131.3] 135.4
2017-01-12 01:19:52,834 set_state: layers/projection_layer/W <- array(10001, 100)
2017-01-12 01:19:52,834 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2017-01-12 01:19:52,835 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2017-01-12 01:19:52,835 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2017-01-12 01:19:52,836 set_state: layers/output_layer/input/W <- array(256, 101)
2017-01-12 01:19:52,836 set_state: layers/output_layer/input/b <- array(101,)
2017-01-12 01:19:52,837 set_state: layers/output_layer/level1/b <- array(101, 100)
2017-01-12 01:19:52,839 set_state: layers/output_layer/level1/W <- array(101, 256, 100)
2017-01-12 01:19:52,841 _reset_state: [1775] (99.83 %) of epoch 4
2017-01-12 01:19:52,841 _log_validation: [1775] Validation set cost history: 168.6 139.2 132.9 [131.3]
2017-01-12 01:19:52,842 set_state: Restored iterator to line 42001 of 42068.
2017-01-12 01:19:52,842 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(101,)
2017-01-12 01:19:52,845 set_state: layers/output_layer/level1/W_gradient <- array(101, 256, 100)
2017-01-12 01:19:52,846 set_state: layers/output_layer/input/W_gradient <- array(256, 101)
2017-01-12 01:19:52,846 set_state: layers/hidden_layer/layer_input/W_sum_sqr_gradient <- array(100, 1024)
2017-01-12 01:19:52,849 set_state: layers/output_layer/level1/W_sum_sqr_gradient <- array(101, 256, 100)
2017-01-12 01:19:52,851 set_state: layers/projection_layer/W_gradient <- array(10001, 100)
2017-01-12 01:19:52,851 set_state: layers/hidden_layer/step_input/W_gradient <- array(256, 1024)
2017-01-12 01:19:52,852 set_state: layers/output_layer/level1/b_sum_sqr_gradient <- array(101, 100)
2017-01-12 01:19:52,852 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 101)
2017-01-12 01:19:52,852 set_state: layers/hidden_layer/step_input/W_sum_sqr_gradient <- array(256, 1024)
2017-01-12 01:19:52,853 set_state: layers/output_layer/input/b_gradient <- array(101,)
2017-01-12 01:19:52,853 set_state: layers/hidden_layer/layer_input/b_sum_sqr_gradient <- array(1024,)
2017-01-12 01:19:52,854 set_state: layers/projection_layer/W_sum_sqr_gradient <- array(10001, 100)
2017-01-12 01:19:52,855 set_state: layers/output_layer/level1/b_gradient <- array(101, 100)
2017-01-12 01:19:52,855 set_state: layers/hidden_layer/layer_input/b_gradient <- array(1024,)
2017-01-12 01:19:52,856 set_state: layers/hidden_layer/layer_input/W_gradient <- array(100, 1024)
Model performance stopped improving. Decreasing learning rate from 0.5 to 0.25 and resetting state to 100 % of epoch 4.
2017-01-12 01:19:52,857 _reset: Generating a random order of input lines.
Finished training epoch 4 in 0 hours 2.5 minutes. Best validation perplexity 131.31.
2017-01-12 01:20:02,891 _log_update: [132] (7.4 %) of epoch 5 -- lr = 0.2, cost = 4.35, duration = 7.5 ms
2017-01-12 01:20:18,081 _log_update: [332] (18.7 %) of epoch 5 -- lr = 0.2, cost = 4.18, duration = 7.5 ms
2017-01-12 01:20:33,273 _log_update: [532] (29.9 %) of epoch 5 -- lr = 0.2, cost = 4.37, duration = 7.5 ms
2017-01-12 01:20:48,471 _log_update: [732] (41.2 %) of epoch 5 -- lr = 0.2, cost = 4.07, duration = 7.6 ms
2017-01-12 01:21:03,663 _log_update: [932] (52.4 %) of epoch 5 -- lr = 0.2, cost = 4.35, duration = 7.5 ms
2017-01-12 01:21:18,857 _log_update: [1132] (63.7 %) of epoch 5 -- lr = 0.2, cost = 3.86, duration = 7.6 ms
2017-01-12 01:21:34,053 _log_update: [1332] (74.9 %) of epoch 5 -- lr = 0.2, cost = 4.13, duration = 7.6 ms
2017-01-12 01:21:49,242 _log_update: [1532] (86.2 %) of epoch 5 -- lr = 0.2, cost = 4.04, duration = 7.5 ms
2017-01-12 01:22:04,433 _log_update: [1732] (97.4 %) of epoch 5 -- lr = 0.2, cost = 4.06, duration = 7.5 ms
2017-01-12 01:22:09,728 _validate: [1772] First validation sample, perplexity 134.24.
2017-01-12 01:22:16,702 _validate: [1775] Center of validation, perplexity 134.42.
2017-01-12 01:22:23,715 _validate: [1778] Last validation sample, perplexity 134.42.
2017-01-12 01:22:23,716 _log_validation: [1778] Validation set cost history: 168.6 139.2 132.9 [131.3] 134.4
2017-01-12 01:22:23,717 set_state: layers/projection_layer/W <- array(10001, 100)
2017-01-12 01:22:23,718 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2017-01-12 01:22:23,718 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2017-01-12 01:22:23,719 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2017-01-12 01:22:23,719 set_state: layers/output_layer/input/W <- array(256, 101)
2017-01-12 01:22:23,719 set_state: layers/output_layer/input/b <- array(101,)
2017-01-12 01:22:23,720 set_state: layers/output_layer/level1/b <- array(101, 100)
2017-01-12 01:22:23,723 set_state: layers/output_layer/level1/W <- array(101, 256, 100)
2017-01-12 01:22:23,724 _reset_state: [1775] (99.83 %) of epoch 4
2017-01-12 01:22:23,724 _log_validation: [1775] Validation set cost history: 168.6 139.2 132.9 [131.3]
2017-01-12 01:22:23,725 set_state: Restored iterator to line 42001 of 42068.
2017-01-12 01:22:23,725 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(101,)
2017-01-12 01:22:23,728 set_state: layers/output_layer/level1/W_gradient <- array(101, 256, 100)
2017-01-12 01:22:23,729 set_state: layers/output_layer/input/W_gradient <- array(256, 101)
2017-01-12 01:22:23,730 set_state: layers/hidden_layer/layer_input/W_sum_sqr_gradient <- array(100, 1024)
2017-01-12 01:22:23,733 set_state: layers/output_layer/level1/W_sum_sqr_gradient <- array(101, 256, 100)
2017-01-12 01:22:23,734 set_state: layers/projection_layer/W_gradient <- array(10001, 100)
2017-01-12 01:22:23,735 set_state: layers/hidden_layer/step_input/W_gradient <- array(256, 1024)
2017-01-12 01:22:23,735 set_state: layers/output_layer/level1/b_sum_sqr_gradient <- array(101, 100)
2017-01-12 01:22:23,735 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 101)
2017-01-12 01:22:23,736 set_state: layers/hidden_layer/step_input/W_sum_sqr_gradient <- array(256, 1024)
2017-01-12 01:22:23,736 set_state: layers/output_layer/input/b_gradient <- array(101,)
2017-01-12 01:22:23,737 set_state: layers/hidden_layer/layer_input/b_sum_sqr_gradient <- array(1024,)
2017-01-12 01:22:23,738 set_state: layers/projection_layer/W_sum_sqr_gradient <- array(10001, 100)
2017-01-12 01:22:23,738 set_state: layers/output_layer/level1/b_gradient <- array(101, 100)
2017-01-12 01:22:23,738 set_state: layers/hidden_layer/layer_input/b_gradient <- array(1024,)
2017-01-12 01:22:23,739 set_state: layers/hidden_layer/layer_input/W_gradient <- array(100, 1024)
Model performance stopped improving. Decreasing learning rate from 0.25 to 0.125 and resetting state to 100 % of epoch 4.
Finished training epoch 4 in 0 hours 2.5 minutes. Best validation perplexity 131.31.
Training finished in 0 hours 17.6 minutes.
2017-01-12 01:22:23,741 set_state: layers/projection_layer/W <- array(10001, 100)
2017-01-12 01:22:23,742 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2017-01-12 01:22:23,742 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2017-01-12 01:22:23,743 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2017-01-12 01:22:23,743 set_state: layers/output_layer/input/W <- array(256, 101)
2017-01-12 01:22:23,743 set_state: layers/output_layer/input/b <- array(101,)
2017-01-12 01:22:23,744 set_state: layers/output_layer/level1/b <- array(101, 100)
2017-01-12 01:22:23,747 set_state: layers/output_layer/level1/W <- array(101, 256, 100)
Best validation set perplexity: 131.298205349
train finished.
Computing evaluation set perplexity.
Reading vocabulary from network state.
Number of words in vocabulary: 10001
Number of word classes: 10001
Building neural network.
Restoring neural network state.
Building text scorer.
Scoring text.
Number of sentences: 3761
Number of words: 86191
Number of out-of-vocabulary words: 4794
Number of predicted probabilities: 82430
Cross entropy (base e): 4.840258273638928
Perplexity: 126.50201964824842
