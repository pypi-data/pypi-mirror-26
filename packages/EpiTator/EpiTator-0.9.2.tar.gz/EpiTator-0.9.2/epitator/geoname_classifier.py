"""
This script was generated by the train.py script in this repository:
https://github.com/ecohealthalliance/geoname-annotator-training
"""
import numpy as np
from numpy import array, int32


HIGH_CONFIDENCE_THRESHOLD = 0.5
GEONAME_SCORE_THRESHOLD = 0.1
base_classifier =\
{'C': 1.0,
 'class_weight': None,
 'classes_': array([False,  True], dtype=bool),
 'coef_': array([[ 0.30886707,  0.02116973,  0.00560847,  0.27765891,  1.94456142,
         0.52508845,  0.18140931,  0.        , -0.95166464,  0.01895766,
        -0.01038021, -0.7187989 , -0.24003574,  2.73676538,  0.        ,
        -0.12539797,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ]]),
 'dual': False,
 'fit_intercept': True,
 'intercept_': array([-11.96377276]),
 'intercept_scaling': 1,
 'max_iter': 100,
 'multi_class': 'ovr',
 'n_iter_': 41,
 'penalty': 'l1',
 'random_state': None,
 'solver': 'liblinear',
 'tol': 0.0001,
 'verbose': 0}
contextual_classifier =\
{'C': 1.0,
 'class_weight': None,
 'classes_': array([False,  True], dtype=bool),
 'coef_': array([[  3.30217878e-01,   2.62288287e-02,   7.08227929e-03,
          2.47995489e-01,   2.13321277e+00,   5.50293856e-01,
          1.12133284e-01,  -4.19516657e+00,  -4.71864549e+00,
         -3.65187897e-03,  -8.64364501e-03,  -3.71959363e+00,
         -3.38073068e+00,   1.12008520e-01,  -2.82198797e+00,
         -1.12271485e-01,   5.41300181e-01,   6.35942072e-01,
         -4.24209965e-01,   3.43790926e-01,  -8.11484961e-01]]),
 'dual': False,
 'fit_intercept': True,
 'intercept_': array([-5.1684584]),
 'intercept_scaling': 1,
 'max_iter': 100,
 'multi_class': 'ovr',
 'n_iter_': 36,
 'penalty': 'l1',
 'random_state': None,
 'solver': 'liblinear',
 'tol': 0.0001,
 'verbose': 0}
# Logistic regression code from scipy
def predict_proba(X, classifier):
    """Probability estimation for OvR logistic regression.
    Positive class probabilities are computed as
    1. / (1. + np.exp(-classifier.decision_function(X)));
    multiclass is handled by normalizing that over all classes.
    """
    prob = np.dot(X, classifier['coef_'].T) + classifier['intercept_']
    prob = prob.ravel() if prob.shape[1] == 1 else prob
    prob *= -1
    np.exp(prob, prob)
    prob += 1
    np.reciprocal(prob, prob)
    if prob.ndim == 1:
        return np.vstack([1 - prob, prob]).T
    else:
        # OvR normalization, like LibLinear's predict_probability
        prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))
        return prob


def predict_proba_base(X):
    return predict_proba(X, base_classifier)


def predict_proba_contextual(X):
    return predict_proba(X, contextual_classifier)
