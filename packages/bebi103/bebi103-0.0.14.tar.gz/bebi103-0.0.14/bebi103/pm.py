import warnings

import numpy as np
import pandas as pd

import pymc3 as pm
import pymc3.stats
import theano.tensor as tt

from .hotdists import *

def trace_to_dataframe(trace, model=None, log_post=False):
    """
    Convert a PyMC3 trace to a Pandas DataFrame

    Parameters
    ----------
    trace : PyMC3 trace
        Trace returned from pm.sample()
    model : PyMC3 model, default None
        Model returned from pm.Model()
    log_post : bool, default False
        If True, also compute the log posterior.

    Returns
    -------
    output : Pandas DataFrame
        DataFrame with samples and various sampling statistics.
    """
    df = pm.trace_to_dataframe(trace, chains=[0])
    for stat in trace.stat_names:
        if stat in df.columns:
            warnings.warn('`' + stat + '` is in the variable names.`'
                          + ' Not adding this statistic.')
        else:
            df[stat] = trace.get_sampler_stats(stat, chains=[0])
    if 'chain' in df.columns:
        warnings.warn('`chain` is in the variable name.`'
                          + ' Not adding this statistic.')
    else:
        df['chain'] = np.array([0]*len(df), dtype=int)

    if trace.nchains > 1:
        for chain in trace.chains[1:]:
            df_app = pm.trace_to_dataframe(trace, chains=[chain])
            for stat in trace.stat_names:
                if stat not in df_app.columns:
                    df_app[stat] = trace.get_sampler_stats(stat,
                                                           chains=[chain])
            if 'chain' not in df_app.columns:
                df_app['chain'] = np.array([chain]*len(df_app))

            df = df.append(df_app, ignore_index=True)

    if log_post:
        # Extract the model from context if necessary
        model = pm.modelcontext(model)

        logp = pymc3.stats._log_post_trace(trace, model).sum(axis=1)
        df['log_posterior'] = logp

    return df


class Jeffreys(pm.Continuous):
    """
    Jeffreys prior for a scale parameter.

    Parameters
    ----------
    lower : float, > 0
        Minimum value the variable can take.
    upper : float, > `lower`
        Maximum value the variable can take.

    Returns
    -------
    output : pymc3 distribution
        Distribution for Jeffreys prior.
    """
    def __init__(self, lower=None, upper=None, transform='interval',
                 *args, **kwargs):
        # Check inputs
        if lower is None or upper is None:
            raise RuntimeError('`lower` and `upper` must be provided.')

        if transform == 'interval':
            transform = pm.distributions.transforms.interval(lower, upper)
        super(Jeffreys, self).__init__(transform=transform, *args, **kwargs)        
        self.lower = lower = pm.theanof.floatX(tt.as_tensor_variable(lower))
        self.upper = upper = pm.theanof.floatX(tt.as_tensor_variable(upper))
        
        self.mean = (upper - lower) / tt.log(upper/lower)
        self.median = tt.sqrt(lower * upper)
        self.mode = lower
        
    def logp(self, value):
        lower = self.lower
        upper = self.upper
        return pm.distributions.dist_math.bound(
                    -tt.log(tt.log(upper/lower)) - tt.log(value),
                    value >= lower, value <= upper)


class MarginalizedHomoscedasticNormal(pm.Continuous):
    """
    Likelihood generated by marginalizing out a homoscedastic variance
    from a Normal distribution.

    Parameters
    ----------
    mu : array
        Mean of the distribution.

    Returns
    -------
    output : pymc3 distribution
        Distribution for a multivariate Gaussian with homoscedastic
        error, normalized over sigma.
    """
    def __init__(self, mu, *args, **kwargs):
        super(MarginalizedHomoscedasticNormal, self).__init__(*args, **kwargs)
        self.mu = mu = tt.as_tensor_variable(mu)
        self.mean = mu
        self.mode = mu


    def logp(self, value):
        n = value.shape[-1]
        prefactor = (  pm.distributions.dist_math.gammaln(n/2)
                     - tt.log(2)
                     - 0.5 * n * tt.log(np.pi)) 
        return prefactor - 0.5 * n * tt.log(tt.sum((value - self.mu)**2))


class GoodBad(pm.Continuous):
    """
    Likelihood for the good-bad data model, in which each data point
    is either "good" with a small variance or "bad" with a large 
    variance.

    Parameters
    ----------
    w : float
        Probability that a data point is "good."
    mu : float
        Mean of the distribution.
    sigma : float
        Standard deviation for "good" data points.
    sigma_bad : float
        Standard deviation for "bad" data points.

    Returns
    -------
    output : pymc3 distribution
        Distribution for the good-bad data model.
    """
    def __init__(self, mu, sigma, sigma_bad, w, *args, **kwargs):
        super(GoodBad, self).__init__(*args, **kwargs)
        self.mu = mu = tt.as_tensor_variable(mu)
        self.sigma = tt.as_tensor_variable(sigma)
        self.sigma_bad = tt.as_tensor_variable(sigma_bad)
        self.w = tt.as_tensor_variable(w)
        self.mean = mu
        self.median = mu
        self.mode = mu

    def logp(self, value):
        prefactor = -tt.log(2.0 * np.pi) / 2.0
        ll_good = (  tt.log(self.w / self.sigma) 
                   - ((value - self.mu) / self.sigma)**2 / 2.0)
        ll_bad = (  tt.log((1.0 - self.w) / self.sigma_bad)
                  - ((value - self.mu) / self.sigma_bad)**2 / 2.0)
        term = tt.switch(tt.gt(ll_good, ll_bad),
                         ll_good + tt.log(1 + tt.exp(ll_bad - ll_good)),
                         ll_bad + tt.log(1 + tt.exp(ll_good - ll_bad)))
        return prefactor + term
    

def ReparametrizedNormal(name, mu=None, sd=None, shape=1):
    """
    Create a reparametrized Normally distributed random variable.

    Parameters
    ----------
    name : str
        Name of the variable.
    mu : float
        Mean of Normal distribution.
    sd : float, > 0
        Standard deviation of Normal distribution.
    shape: int or tuple of ints, default 1
        Shape of array of variables. If 1, then a single scalar.

    Returns
    -------
    output : pymc3 distribution
        Distribution for a reparametrized Normal distribution.

    Notes
    -----
    .. The reparametrization procedure allows the sampler to sample
       a standard normal distribution, and then do a deterministic
       reparametrization to achieve sampling of the original desired 
       Normal distribution.
    """
    # Check inputs
    if type(name) != str:
        raise RuntimeError('`name` must be a string.')
    if mu is None or sd is None:
        raise RuntimeError('`mu` and `sd` must be provided.')

    var_reparam = pm.Normal(name + '_reparam', mu=0, sd=1, shape=shape)
    var = pm.Deterministic(name, mu + var_reparam * sd)

    return var


def ReparametrizedCauchy(name, alpha=None, beta=None, shape=1):
    """
    Create a reparametrized Cauchy distributed random variable.

    Parameters
    ----------
    name : str
        Name of the variable.
    alpha : float
        Mode of Cauchy distribution.
    beta : float, > 0
        Scale parameter of Cauchy distribution
    shape: int or tuple of ints, default 1
        Shape of array of variables. If 1, then a single scalar.

    Returns
    -------
    output : pymc3 distribution
        Reparametrized Cauchy distribution.

    Notes
    -----
    .. The reparametrization procedure allows the sampler to sample
       a Cauchy distribution with alpha = 0 and beta = 1, and then do a
       deterministic reparametrization to achieve sampling of the 
       original desired Cauchy distribution.
    """
    # Check inputs
    if type(name) != str:
        raise RuntimeError('`name` must be a string.')
    if alpha is None or beta is None:
        raise RuntimeError('`alpha` and `beta` must be provided.')

    var_reparam = pm.Cauchy(name + '_reparam', alpha=0, beta=1, shape=shape)
    var = pm.Deterministic(name, alpha + var_reparam * beta)

    return var


class Ordered(pm.distributions.transforms.ElemwiseTransform):
    """
    Class defining transform to order entries in an array.

    Code from Adrian Seyboldt from PyMC3 discourse: https://discourse.pymc.io/t/mixture-models-and-breaking-class-symmetry/208/4
    """
    name = 'ordered'

    def forward(self, x):
        out = tt.zeros(x.shape)
        out = tt.inc_subtensor(out[0], x[0])
        out = tt.inc_subtensor(out[1:], tt.log(x[1:] - x[:-1]))
        return out
    
    def forward_val(self, x, point=None):
        x, = pm.distributions.distribution.draw_values([x], point=point)
        return self.forward(x)

    def backward(self, y):
        out = tt.zeros(y.shape)
        out = tt.inc_subtensor(out[0], y[0])
        out = tt.inc_subtensor(out[1:], tt.exp(y[1:]))
        return tt.cumsum(out)

    def jacobian_det(self, y):
        return tt.sum(y[1:])


class Composed(pm.distributions.transforms.Transform):    
    """
    Class to build a transform out of an elementwise transform.

    Code from Adrian Seyboldt from PyMC3 discourse: https://discourse.pymc.io/t/mixture-models-and-breaking-class-symmetry/208/4   
    """
    def __init__(self, trafo1, trafo2):
        self._trafo1 = trafo1
        self._trafo2 = trafo2
        self.name = '_'.join([trafo1.name, trafo2.name])

    def forward(self, x):
        return self._trafo2.forward(self._trafo1.forward(x))
    
    def forward_val(self, x, point=None):
        return self.forward(x)

    def backward(self, y):
        return self._trafo1.backward(self._trafo2.backward(y))

    def jacobian_det(self, y):
        y2 = self._trafo2.backward(y)
        det1 = self._trafo1.jacobian_det(y2)
        det2 = self._trafo2.jacobian_det(y)
        return det1 + det2


def ordered_transform():
    """
    Make an ordered transform.

    Returns
    -------
    output : pm.distirbutions.transforms.Transform subclass instance
        Transform to order entries in tensor.

    Example
    -------
    To insist on ordering probabilities, p1 <= p2 <= p3,
    >>> p = pymc3.Beta('p',
                       alpha=1,
                       beta=1,
                       shape=3,
                       transform=ordered_transform())
    """
    return Composed(pm.distributions.transforms.LogOdds(), Ordered())


def hotdist(dist, name, beta, *args, **kwargs):
    """
    Instantiate a "hot" distribution. The "hot" distribution takes the
    value returned by the logp method of `dist` and returns beta * logp.

    Parameters
    ----------
    dist : PyMC3 distribution
        The name of a distribution you want to make hot. Examples:
        pm.Normal, pm.Binomial, pm.MvNormal, pm.Dirichlet.
    name : str
        Name of the random variable.
    beta : float on interval [0, 1]
        Beta value (inverse temperature) of the distribution.

    Returns
    -------
    output : pymc3 distribution
        Hot distribution. 
    """
    class HotDistribution(dist):
        def __init__(self, beta, *args, **kwargs):
            super(HotDistribution, self).__init__(*args, **kwargs)
            if not (0 <= beta <= 1):
                raise RuntimeError('Must have 0 ≤ beta ≤ 1.')
            self.beta = beta

        def logp(self, value):
            return self.beta * dist.logp(self, value)
        
    return HotDistribution(name, beta, *args, **kwargs)


def chol_to_cov(chol, cov_prefix):
    """
    Convert flattened Cholesky matrix to covariance.

    Parameters
    ----------
    chol : array_like
        Lexicographically flattened Cholesky decomposition as returned
        from trace.get_values(chol), where trace is a PyMC3 MultiTrace
        instance.
    chol_prefix : str
        Prefix for the nam e of the covariance variable. Results are
        stored as prefix__i__j, where i and j are the row and column
        indices, respectively.

    Returns
    -------
    output : Pandas DataFrame
        DataFrame with values of samples of the components of the
        covariance matrix.
    """
    chol = np.array(chol)

    n = int(np.round((-1 + np.sqrt(8*chol.shape[1] + 1)) / 2))
    sigma = np.zeros_like(chol)
    inds = np.tril_indices(n)
    for i, r in enumerate(chol):
        L =  np.zeros((n, n))
        L[inds] = r
        sig = np.dot(L, L.T)
        sigma[i] = sig[inds]

    cols = ['{0:s}__{1:d}__{2:d}'.format(cov_prefix, i, j) 
                for i, j in zip(*inds)]
    return pd.DataFrame(columns=cols, data=sigma)


def hpd(x, mass_frac) :
    """
    Returns highest probability density region given by
    a set of samples.

    Parameters
    ----------
    x : array
        1D array of MCMC samples for a single variable
    mass_frac : float with 0 < mass_frac <= 1
        The fraction of the probability to be included in
        the HPD.  For example, `massfrac` = 0.95 gives a
        95% HPD.
        
    Returns
    -------
    output : array, shape (2,)
        The bounds of the HPD
    """
    # Get sorted list
    d = np.sort(np.copy(x))

    # Number of total samples taken
    n = len(x)
    
    # Get number of samples that should be included in HPD
    n_samples = np.floor(mass_frac * n).astype(int)
    
    # Get width (in units of data) of all intervals with n_samples samples
    int_width = d[n_samples:] - d[:n-n_samples]
    
    # Pick out minimal interval
    min_int = np.argmin(int_width)
    
    # Return interval
    return np.array([d[min_int], d[min_int+n_samples]])
